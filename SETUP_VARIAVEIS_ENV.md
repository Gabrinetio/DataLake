# üîí Transforma√ß√£o de Credenciais em Vari√°veis de Ambiente

**Status:** ‚úÖ COMPLETO (08/12/2025)

---

## üìã O Que Foi Feito

### 1. ‚úÖ Remover Senhas do C√≥digo

| Arquivo | Antes | Depois |
|---------|-------|--------|
| `docs/Projeto.md` | `S3cureHivePass2025` | `${HIVE_DB_PASSWORD}` |
| `src/config.py` | ‚ùå N√£o existia | ‚úÖ Criado com config centralizada |
| M√∫ltiplos scripts Python | Hardcoded `SparkPass123!` | ‚ö†Ô∏è Ainda precisam ser atualizados |

### 2. ‚úÖ Criar Template de Vari√°veis

**Arquivo:** `.env.example` (versionado)
- 60+ linhas com coment√°rios explicativos
- Todas as vari√°veis necess√°rias documentadas
- Placeholders `<SUA_...>` em vez de senhas reais
- Se√ß√µes organizadas: Hive, MinIO, Spark, Kafka, etc.

### 3. ‚úÖ Criar M√≥dulo de Configura√ß√£o Python

**Arquivo:** `src/config.py` (novo)
- Carrega `.env` automaticamente
- Valida√ß√£o de credenciais obrigat√≥rias
- Helper functions: `get_hive_jdbc_url()`, `get_spark_s3_config()`
- Teste integrado: `python -m src.config`

### 4. ‚úÖ Scripts de Carregamento de Vari√°veis

**PowerShell:** `load_env.ps1`
```powershell
. .\load_env.ps1
```

**Bash/Linux:** `load_env.sh`
```bash
source ./load_env.sh
```

### 5. ‚úÖ Documenta√ß√£o Completa

**Arquivo:** `docs/VARI√ÅVEIS_ENV.md` (novo - 200+ linhas)
- Setup inicial por SO (Windows, Linux, macOS)
- Como preencher vari√°veis
- Exemplos de uso em Python, Bash, PowerShell
- Boas pr√°ticas e troubleshooting
- Refer√™ncias para produ√ß√£o (Vault, AWS Secrets)

### 6. ‚úÖ Atualizar .gitignore

**Prote√ß√£o:**
```
.env                    ‚Üê Arquivo com credenciais reais (N√ÉO versionado)
.env.local
.env.*.local
*.key, *.pem, *.crt     ‚Üê Certificados e chaves
```

### 7. ‚úÖ Atualizar CONTEXT.md

**Nova Se√ß√£o 13:** "üîí Gest√£o de Credenciais & Vari√°veis de Ambiente"
- Estrat√©gia explicada
- Regras cr√≠ticas
- Links para documenta√ß√£o detalhada

---

## üöÄ Como Usar Agora

### Setup Inicial (Fa√ßa uma vez)

#### Windows PowerShell
```powershell
# 1. Copiar template
Copy-Item .env.example .env

# 2. Editar com suas credenciais reais
code .env

# 3. Carregar vari√°veis (em cada terminal)
. .\load_env.ps1
```

#### Linux/macOS
```bash
# 1. Copiar template
cp .env.example .env

# 2. Editar com suas credenciais
nano .env

# 3. Carregar vari√°veis
source .env
```

### Usar em Scripts Python

**ANTES (‚ùå ERRADO):**
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .config("spark.hadoop.fs.s3a.secret.key", "iRB;g2&ChZ&XQEW!") \
    .getOrCreate()
```

**DEPOIS (‚úÖ CORRETO):**
```python
from src.config import get_spark_s3_config

spark = SparkSession.builder \
    .config(get_spark_s3_config()) \
    .getOrCreate()
```

### Verificar Configura√ß√£o

```bash
# Python
python -m src.config

# PowerShell
. .\load_env.ps1
echo "Host: $env:HIVE_DB_HOST"

# Bash
source .env
echo "Host: $HIVE_DB_HOST"
```

---

## üìä Vari√°veis Dispon√≠veis

### Hive Metastore
```
HIVE_DB_HOST             = localhost
HIVE_DB_PORT             = 3306
HIVE_DB_NAME             = metastore
HIVE_DB_USER             = hive
HIVE_DB_PASSWORD         = ‚ö†Ô∏è Configure aqui
```

### MinIO / S3A
```
S3A_ACCESS_KEY           = datalake
S3A_SECRET_KEY           = ‚ö†Ô∏è Configure aqui
S3A_ENDPOINT             = http://minio.gti.local:9000
S3A_PATH_STYLE_ACCESS    = true
```

### Spark
```
SPARK_WAREHOUSE_PATH     = s3a://datalake/warehouse
SPARK_S3A_SECRET_KEY     = (mesmo de S3A_SECRET_KEY)
```

### Kafka
```
KAFKA_BROKER             = kafka.gti.local:9092
KAFKA_SECURITY_PROTOCOL  = PLAINTEXT
```

---

## ‚úÖ Verifica√ß√£o

- [x] `.env.example` criado com template completo
- [x] `.env` adicionado a `.gitignore` (seguran√ßa)
- [x] `src/config.py` com carregamento autom√°tico
- [x] Scripts PowerShell e Bash para carregar vari√°veis
- [x] Documenta√ß√£o completa em `docs/VARI√ÅVEIS_ENV.md`
- [x] CONTEXT.md atualizado com nova se√ß√£o
- [x] Projeto.md com ejemplo de uso (${}format)
- [x] ‚ö†Ô∏è TODO: Atualizar scripts Python individuais para usar `src.config`

---

## üìù Pr√≥ximos Passos (TODO)

### [Alta Prioridade]
1. **Atualizar todos os scripts Python** para usar `from src.config import ...`
   - `src/tests/test_*.py` (15+ arquivos)
   - `src/test_iceberg_partitioned.py`
   - Scripts de exemplo

2. **Atualizar scripts shell** para usar `source .env`
   - `etc/scripts/*.sh`
   - `*.sh` na raiz

3. **Criar `.env` no servidor**
   - Via SSH: `scp .env datalake@spark.gti.local:/home/datalake/`
   - Ou criar manualmente: `source /home/datalake/.env`

### [M√©dia Prioridade]
4. **Produ√ß√£o:** Integrar com Vault/AWS Secrets Manager
5. **CI/CD:** Injetar vari√°veis no pipeline (GitHub Actions, GitLab CI, etc.)
6. **Valida√ß√£o:** Testes para garantir que nenhuma senha seja exposta em logs

### [Opcional]
7. Pre-commit hook para verificar commits de `.env`
8. Script de auditoria para encontrar hardcoded passwords

---

## üîê Seguran√ßa - Lembretes

‚úÖ Nunca commitar `.env`
‚úÖ Nunca enviar `.env` por email/chat
‚úÖ Nunca fazer push de `.env` para reposit√≥rio
‚úÖ Sempre usar `source .env` ou script de carregamento
‚úÖ Revisar logs para garantir que senhas n√£o sejam expostas
‚úÖ Rotacionar senhas regularmente

---

**Para detalhes completos, leia:** [`docs/VARI√ÅVEIS_ENV.md`](docs/VARI√ÅVEIS_ENV.md)

