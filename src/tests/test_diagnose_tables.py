#!/usr/bin/env python3
"""
Diagn√≥stico de Tabelas no Cat√°logo Iceberg
Verifica que tabelas est√£o dispon√≠veis e como acess√°-las
"""

import sys
import json
import traceback
from datetime import datetime
from src.config import get_spark_s3_config

def diagnose_tables():
    """Diagnostica a disponibilidade de tabelas no cluster Spark"""
    
    results = {
        "timestamp": datetime.now().isoformat(),
        "diagnosis": [],
        "errors": [],
        "recommendations": []
    }
    
    try:
        from pyspark.sql import SparkSession
        
        # Tentar criar SparkSession com Iceberg
        print("\n" + "="*70)
        print("üîç DIAGN√ìSTICO DE TABELAS ICEBERG")
        print("="*70 + "\n")
        
        print("1Ô∏è‚É£  Criando SparkSession COM extens√µes Iceberg...")
        try:
            spark_config = get_spark_s3_config()
            spark = SparkSession.builder \
                .appName("Diagnose_Tables_v1") \
                .master("local[2]") \
                .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
                .config("spark.sql.catalog.hadoop_prod", "org.apache.iceberg.spark.SparkCatalog") \
                .config("spark.sql.catalog.hadoop_prod.type", "hadoop") \
                .config("spark.sql.catalog.hadoop_prod.warehouse", "s3a://datalake/warehouse") \
                .getOrCreate()
            
            print("   ‚úÖ SparkSession com Iceberg criada com sucesso")
            results["diagnosis"].append({
                "step": "SparkSession with Iceberg extensions",
                "status": "SUCCESS"
            })
            
            # Listar cat√°logos
            print("\n2Ô∏è‚É£  Listando cat√°logos dispon√≠veis...")
            try:
                catalogs = spark.sql("SHOW CATALOGS").collect()
                print(f"   üìö Cat√°logos encontrados: {len(catalogs)}")
                for catalog in catalogs:
                    print(f"      ‚Ä¢ {catalog[0]}")
                    results["diagnosis"].append({
                        "step": "SHOW CATALOGS",
                        "catalog": catalog[0]
                    })
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Erro ao listar cat√°logos: {e}")
                results["errors"].append(f"SHOW CATALOGS error: {str(e)}")
            
            # Listar esquemas no cat√°logo hadoop_prod
            print("\n3Ô∏è‚É£  Listando esquemas em hadoop_prod...")
            try:
                schemas = spark.sql("SHOW SCHEMAS IN hadoop_prod").collect()
                print(f"   üìÇ Esquemas encontrados: {len(schemas)}")
                for schema in schemas:
                    print(f"      ‚Ä¢ {schema[0]}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Erro ao listar esquemas: {e}")
                results["errors"].append(f"SHOW SCHEMAS error: {str(e)}")
            
            # Listar tabelas no default
            print("\n4Ô∏è‚É£  Listando tabelas em hadoop_prod.default...")
            try:
                tables = spark.sql("SHOW TABLES IN hadoop_prod.default").collect()
                print(f"   üìã Tabelas encontradas: {len(tables)}")
                for table in tables:
                    print(f"      ‚Ä¢ {table[0]}.{table[1]}")
                    results["diagnosis"].append({
                        "step": "SHOW TABLES",
                        "table": f"{table[0]}.{table[1]}"
                    })
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Erro ao listar tabelas: {e}")
                results["errors"].append(f"SHOW TABLES error: {str(e)}")
                
                # Tentar alternativa: listar arquivos no S3
                print("\n   ‚ÑπÔ∏è  Tentando alternativa: listar arquivos no S3...")
                try:
                    files = spark.sparkContext.binaryFiles("s3a://datalake/warehouse/default/vendas_small/").take(5)
                    print(f"   üìÅ Arquivos encontrados em S3: {len(files)}")
                    results["diagnosis"].append({
                        "step": "S3 files listing",
                        "files_found": len(files)
                    })
                except Exception as e2:
                    print(f"   ‚ö†Ô∏è  Erro ao listar arquivos S3: {e2}")
            
            # Tentar usar tabela diretamente
            print("\n5Ô∏è‚É£  Tentando acessar vendas_small com diferentes qualifica√ß√µes...")
            
            table_names = [
                "hadoop_prod.default.vendas_small",
                "default.vendas_small",
                "vendas_small",
                "hadoop_prod.vendas_small"
            ]
            
            for table_name in table_names:
                try:
                    count = spark.sql(f"SELECT COUNT(*) FROM {table_name}").collect()[0][0]
                    print(f"   ‚úÖ {table_name}: {count} registros")
                    results["diagnosis"].append({
                        "step": "Table access",
                        "table": table_name,
                        "status": "SUCCESS",
                        "row_count": count
                    })
                except Exception as e:
                    print(f"   ‚ùå {table_name}: ERRO")
                    results["diagnosis"].append({
                        "step": "Table access",
                        "table": table_name,
                        "status": "FAILED",
                        "error": str(e)[:100]
                    })
            
            spark.stop()
            
        except Exception as e:
            print(f"   ‚ùå Erro criando SparkSession com Iceberg: {e}")
            results["errors"].append(f"SparkSession creation error: {str(e)}")
            
            # Tentar criar SparkSession SEM Iceberg
            print("\n6Ô∏è‚É£  Tentando SparkSession SEM extens√µes Iceberg...")
            try:
                spark_config = get_spark_s3_config()
                spark = SparkSession.builder \
                    .appName("Diagnose_Tables_v2") \
                    .master("local[2]") \
                    .getOrCreate()
                
                print("   ‚úÖ SparkSession sem Iceberg criada")
                
                # Tentar listar arquivo Parquet direto
                print("\n   Tentando ler Parquet de s3a://datalake/warehouse/default/vendas_small/...")
                try:
                    df = spark.read.parquet("s3a://datalake/warehouse/default/vendas_small/")
                    count = df.count()
                    print(f"   ‚úÖ Leitura bem-sucedida: {count} registros")
                    print(f"      Schema: {df.schema}")
                    results["diagnosis"].append({
                        "step": "Direct Parquet read",
                        "status": "SUCCESS",
                        "row_count": count
                    })
                except Exception as e2:
                    print(f"   ‚ùå Erro lendo Parquet: {e2}")
                    results["errors"].append(f"Parquet read error: {str(e2)}")
                
                spark.stop()
                
            except Exception as e2:
                print(f"   ‚ùå Erro criando SparkSession sem Iceberg: {e2}")
                results["errors"].append(f"SparkSession without Iceberg error: {str(e2)}")
        
    except Exception as e:
        print(f"\n‚ùå ERRO GERAL: {e}")
        traceback.print_exc()
        results["errors"].append(f"General error: {str(e)}")
    
    # Gerar recomenda√ß√µes
    print("\n" + "="*70)
    print("üí° RECOMENDA√á√ïES")
    print("="*70 + "\n")
    
    if any("TABLE_OR_VIEW_NOT_FOUND" in str(err) for err in results["errors"]):
        rec = "Tabela n√£o encontrada. Pode ser que:\n  - A tabela n√£o foi criada corretamente\n  - A tabela est√° em outro cat√°logo/schema\n  - Os dados est√£o em outro local no S3"
        print(f"   {rec}")
        results["recommendations"].append(rec)
    
    if any("Cannot find catalog plugin" in str(err) for err in results["errors"]):
        rec = "Cat√°logo Iceberg n√£o configurado corretamente. Verificar:\n  - Iceberg JAR no classpath\n  - Configura√ß√£o de spark.sql.catalog.hadoop_prod\n  - Vers√£o compat√≠vel do Spark"
        print(f"   {rec}")
        results["recommendations"].append(rec)
    
    # Salvar resultados
    output_file = "/tmp/diagnose_tables_results.json"
    with open(output_file, "w") as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\n‚úÖ Resultados salvos em: {output_file}\n")
    
    return results

if __name__ == "__main__":
    diagnose_tables()
