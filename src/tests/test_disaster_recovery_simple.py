#!/usr/bin/env python3
"""
Disaster Recovery Procedimento
Para Iteration 4 - Production Hardening
Cria checkpoint, simula corrup√ß√£o, recupera, valida
"""

import json
import traceback
from datetime import datetime
from pyspark.sql import SparkSession
import os
import shutil

def create_checkpoint(spark, data_path, checkpoint_name):
    """Cria checkpoint dos dados atuais"""
    
    print(f"\nüì∏ Criando checkpoint: {checkpoint_name}...")
    
    try:
        checkpoint_dir = f"/home/datalake/checkpoints/{checkpoint_name}"
        
        # Remover se j√° existe
        os.system(f"rm -rf {checkpoint_dir}")
        os.system(f"mkdir -p {checkpoint_dir}")
        
        # Copiar dados
        df = spark.read.parquet(data_path)
        count = df.count()
        
        df.write \
            .mode("overwrite") \
            .parquet(checkpoint_dir)
        
        print(f"‚úÖ Checkpoint criado: {checkpoint_name}")
        print(f"‚úì Registros: {count}")
        
        return True, count, checkpoint_dir
        
    except Exception as e:
        print(f"‚ùå Erro criando checkpoint: {e}")
        return False, 0, None

def simulate_data_corruption(spark, data_path):
    """Simula corrup√ß√£o de dados (adiciona registros inv√°lidos)"""
    
    print(f"\n‚ö†Ô∏è  Simulando corrup√ß√£o de dados...")
    
    try:
        from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType, StringType, DoubleType
        from pyspark.sql.functions import lit
        from datetime import datetime as dt
        
        # Ler dados
        df = spark.read.parquet(data_path)
        
        # Criar registros corrompidos (com valores NULL em campos cr√≠ticos)
        schema = df.schema
        corrupt_records = [
            {
                "id": 99999,
                "data_venda": dt(2999, 12, 31),
                "categoria": "CORRUPTO",
                "produto": None,  # NULL em campo que n√£o deveria
                "quantidade": -999,  # Quantidade negativa (inv√°lida)
                "preco_unitario": None,  # NULL em pre√ßo
                "total": -9999.0,  # Valor negativo (inv√°lido)
            },
            {
                "id": 100000,
                "data_venda": None,  # NULL em data
                "categoria": "CORRUPTO",
                "produto": "Produto_Corrompido",
                "quantidade": 0,  # Quantidade zero
                "preco_unitario": 0.0,
                "total": 0.0,
            }
        ]
        
        df_corrupt = spark.createDataFrame(corrupt_records, schema)
        
        # Unir dados originais com corrompidos
        df_combined = df.unionByName(df_corrupt)
        
        # Salvar dados corrompidos
        df_combined.write \
            .mode("overwrite") \
            .parquet(data_path)
        
        new_count = df_combined.count()
        print(f"‚úì Dados corrompidos: {new_count} registros (adicionados 2 inv√°lidos)")
        
        return True, new_count
        
    except Exception as e:
        print(f"‚ùå Erro simulando corrup√ß√£o: {e}")
        traceback.print_exc()
        return False, 0

def recover_to_checkpoint(spark, checkpoint_dir, recovery_path):
    """Restaura dados do checkpoint (recupera√ß√£o de DR)"""
    
    print(f"\nüîÑ Restaurando dados do checkpoint...")
    
    try:
        # Remover dados corrupto
        os.system(f"rm -rf {recovery_path}/*")
        
        # Copiar checkpoint
        df_checkpoint = spark.read.parquet(checkpoint_dir)
        count = df_checkpoint.count()
        
        df_checkpoint.write \
            .mode("overwrite") \
            .parquet(recovery_path)
        
        print(f"‚úÖ Recupera√ß√£o completada")
        print(f"‚úì Registros restaurados: {count}")
        
        return True, count
        
    except Exception as e:
        print(f"‚ùå Erro recuperando: {e}")
        return False, 0

def validate_recovery(spark, recovery_path, original_count):
    """Valida integridade ap√≥s recupera√ß√£o"""
    
    print(f"\n‚úì Validando recupera√ß√£o...")
    
    try:
        df_recovered = spark.read.parquet(recovery_path)
        recovered_count = df_recovered.count()
        
        print(f"Contagem original: {original_count}")
        print(f"Contagem recuperada: {recovered_count}")
        
        # Validar contagem
        if recovered_count == original_count:
            print(f"‚úÖ Contagem validada")
        else:
            print(f"‚ö†Ô∏è  Aviso: contagens diferentes")
        
        # Validar estrutura
        try:
            # Verificar se n√£o h√° valores inv√°lidos
            invalid_count = df_recovered.filter(
                (df_recovered.quantidade < 0) |
                (df_recovered.preco_unitario < 0) |
                (df_recovered.total < 0)
            ).count()
            
            print(f"Registros inv√°lidos: {invalid_count}")
            
            if invalid_count == 0:
                print(f"‚úÖ Dados v√°lidos")
                return True
            else:
                print(f"‚ö†Ô∏è  Dados ainda cont√™m registros inv√°lidos")
                return False
                
        except:
            # Se valida√ß√£o falhar, apenas retorna true baseado na contagem
            return recovered_count == original_count
        
    except Exception as e:
        print(f"‚ùå Erro validando: {e}")
        return False

def run():
    """Executa procedimento de disaster recovery"""
    
    print("\n" + "="*70)
    print("üö® DISASTER RECOVERY PROCEDIMENTO - ITERATION 4")
    print("="*70)
    
    results = {
        "timestamp": datetime.now().isoformat(),
        "steps": [],
        "summary": {},
        "errors": []
    }
    
    try:
        # 1. SparkSession
        print("\nüîß Iniciando SparkSession...")
        
        spark = SparkSession.builder \
            .appName("DisasterRecovery") \
            .master("local[2]") \
            .getOrCreate()
        
        print("‚úÖ SparkSession criada")
        
        # 2. Criar checkpoint
        print("\n" + "="*70)
        print("FASE 1: CRIA√á√ÉO DE CHECKPOINT")
        print("="*70)
        
        data_path = "/home/datalake/data/vendas_small"
        checkpoint_name = f"checkpoint_{int(datetime.now().timestamp())}"
        
        success, orig_count, checkpoint_dir = create_checkpoint(spark, data_path, checkpoint_name)
        
        if not success:
            raise Exception("Falha ao criar checkpoint")
        
        results["steps"].append({
            "step": "Checkpoint creation",
            "status": "SUCCESS",
            "checkpoint_name": checkpoint_name,
            "records": orig_count
        })
        
        # 3. Simular corrup√ß√£o
        print("\n" + "="*70)
        print("FASE 2: SIMULA√á√ÉO DE CORRUP√á√ÉO")
        print("="*70)
        
        success, corrupt_count = simulate_data_corruption(spark, data_path)
        
        if not success:
            raise Exception("Falha ao simular corrup√ß√£o")
        
        results["steps"].append({
            "step": "Data corruption simulation",
            "status": "SUCCESS",
            "corrupted_records": corrupt_count
        })
        
        # 4. Recuperar do checkpoint
        print("\n" + "="*70)
        print("FASE 3: RECUPERA√á√ÉO")
        print("="*70)
        
        recovery_path = "/home/datalake/data/vendas_small_recovered"
        os.system(f"mkdir -p {recovery_path}")
        
        success, recovered_count = recover_to_checkpoint(spark, checkpoint_dir, recovery_path)
        
        if not success:
            raise Exception("Falha na recupera√ß√£o")
        
        results["steps"].append({
            "step": "Recovery from checkpoint",
            "status": "SUCCESS",
            "recovered_records": recovered_count
        })
        
        # 5. Validar recupera√ß√£o
        print("\n" + "="*70)
        print("FASE 4: VALIDA√á√ÉO")
        print("="*70)
        
        recovery_valid = validate_recovery(spark, recovery_path, orig_count)
        
        results["steps"].append({
            "step": "Recovery validation",
            "status": "SUCCESS" if recovery_valid else "WARNING",
            "original_count": orig_count,
            "recovered_count": recovered_count,
            "validation_passed": recovery_valid
        })
        
        # Resumo
        print("\n" + "="*70)
        print("üìã RESUMO DO DISASTER RECOVERY")
        print("="*70)
        print(f"‚úÖ Checkpoint criado: {checkpoint_name}")
        print(f"‚úÖ Corrup√ß√£o simulada: 2 registros adicionados")
        print(f"‚úÖ Recupera√ß√£o: {recovered_count} registros")
        print(f"‚úÖ Valida√ß√£o: {'PASSOU ‚úì' if recovery_valid else 'FALHOU ‚úó'}")
        
        results["summary"] = {
            "checkpoint_name": checkpoint_name,
            "original_records": orig_count,
            "corrupted_records": corrupt_count,
            "recovered_records": recovered_count,
            "recovery_valid": recovery_valid,
            "status": "SUCCESS"
        }
        
        spark.stop()
        
    except Exception as e:
        print(f"\n‚ùå ERRO: {e}")
        traceback.print_exc()
        results["errors"].append(str(e))
        results["summary"]["status"] = "FAILED"
    
    # Salvar resultados
    output_file = "/tmp/disaster_recovery_results.json"
    with open(output_file, "w") as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\n‚úÖ Resultados salvos: {output_file}\n")

if __name__ == "__main__":
    run()
