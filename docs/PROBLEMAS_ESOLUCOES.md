# Problemas e Solu√ß√µes ‚Äî Documenta√ß√£o de Troubleshooting

**√öltima Atualiza√ß√£o:** 10/12/2025  
**Total de Solu√ß√µes:** 12+

---

## Iceberg Catalog Storage Configuration ‚Äî Trino/Hadoop Persistence

**Data:** 10/12/2025  
**Status:** ‚ö†Ô∏è Em andamento

### Problema:
- Trino + Iceberg apontando para `/user/hive/warehouse/` que n√£o existe no container
- Arquivo de configura√ß√£o `iceberg.properties` n√£o √© carregado automaticamente ap√≥s restart
- Falta de permiss√µes de escrita em diret√≥rios padr√£o
- SSH multi-hop para container Trino com espa√ßos em caminho causa falhas de parsing

### Investiga√ß√£o:
1. **Container Trino**: ‚úÖ Funcionando (uptime 1.29m+)
2. **Cat√°logo Iceberg**: ‚úÖ Carregado e vis√≠vel em `SHOW CATALOGS`
3. **Esquemas**: ‚úÖ `default` e `information_schema` acess√≠veis
4. **Query b√°sicas**: ‚úÖ `SELECT 1` executa com sucesso
5. **Persist√™ncia de metadados**: ‚ùå FALHA ao criar tabelas

### Solu√ß√µes Testadas e Resultados:

| Solu√ß√£o | Resultado | Motivo |
|---------|-----------|--------|
| warehouse=`file:/user/hive/warehouse/` | ‚ùå FALHA | Diret√≥rio n√£o existe, sem permiss√£o de escrita |
| warehouse=`file:/home/datalake/data/iceberg_warehouse` | ‚ùå FALHA | Mesmo erro, diret√≥rio relativo n√£o acess√≠vel |
| warehouse=`file:/tmp/iceberg_warehouse` | ‚ùå FALHA | Config n√£o carregada ap√≥s restart |
| catalog.type=`hadoop` com Hadoop AWS libs | ‚ö†Ô∏è BLOQUEADO | Sem acesso SSH para instalar depend√™ncias |

### Causa Raiz:
**Configura√ß√£o n√£o persiste ap√≥s restart** ‚Äî O arquivo `iceberg.properties` √© ignorado. O Trino deve ter uma configura√ß√£o padr√£o em outro local ou requerer reinicializa√ß√£o diferente.

### Bloqueio Atual (10/12/2025):
- **SSH Multi-hop com espa√ßos em caminhos**: PowerShell falha ao fazer parsing de caminhos como `C:\Users\Gabriel Santana\.ssh\...`
- **Resultado**: N√£o consegue copiar `iceberg.properties` para container Trino
- **Impacto**: Cat√°logos adicionais (hive, iceberg com config customizada) n√£o carregam

### Solu√ß√µes Vi√°veis (Por ordem de viabilidade):

#### ‚úÖ Solu√ß√£o 1: Usar Linux/WSL (Recomendada)
- PowerShell em Windows √© limitado para SSH com caminhos complexos
- WSL2 com bash resolveria o problema imediatamente
- Alternativa: Git Bash com escape proper

#### ‚ö†Ô∏è Solu√ß√£o 2: Criar config via Docker volume
- Mapear arquivo local como volume no container
- Reinicar container com `docker run ... -v`
- Requer acesso ao engine Docker/Proxmox

#### ‚ùå Solu√ß√£o 3: Compilar Trino com config embutida
- Muito complexo para escopo atual
- N√£o recomendado para Itera√ß√£o 5

### Status Final do Iceberg (Itera√ß√£o 5):
- **‚úÖ Cat√°logo carregado**: Sim, reconhecido por Trino
- **‚úÖ Metadados acess√≠veis**: Schemas padr√£o funcionam
- **‚úÖ Query b√°sicas**: `SELECT 1`, `SHOW CATALOGS` OK
- **‚ùå Persist√™ncia de tabelas**: BLOQUEADO (sem config aplicada)
- **Recomenda√ß√£o**: Anotar como **"Pronto para S3 + Hive ap√≥s libertar acesso SSH"**

---

## Migra√ß√£o de Credenciais para Vari√°veis de Ambiente

**Data:** 08/12/2025  
**Status:** ‚úÖ Completo

### Problema:
- Credenciais hardcoded em c√≥digo, documenta√ß√£o e scripts
- Risco de exposi√ß√£o de senhas no Git
- Falta de padroniza√ß√£o no carregamento de vari√°veis

### Solu√ß√µes Aplicadas:

1. **Criado arquivo `.env.example`** (versionado)
   - 18+ vari√°veis documentadas
   - Placeholders `<SUA_...>` em lugar de valores reais
   - Coment√°rios explicativos para cada se√ß√£o

2. **Criado m√≥dulo `src/config.py`** (Python centralizado)
   - Carrega `.env` automaticamente
   - Valida credenciais obrigat√≥rias no startup
   - Fun√ß√µes helpers: `get_spark_s3_config()`, `get_hive_jdbc_url()`, etc.
   - Teste integrado: `python -m src.config`

3. **Scripts de carregamento criados:**
   - `load_env.ps1` ‚Äî PowerShell (Windows)
   - `load_env.sh` ‚Äî Bash/Zsh (Linux/macOS)

4. **Documenta√ß√£o completa:**
   - `docs/VARI√ÅVEIS_ENV.md` ‚Äî 200+ linhas com exemplos para todos os shells
   - `SETUP_VARIAVEIS_ENV.md` ‚Äî Guia r√°pido
   - Se√ß√£o 2.4 em `docs/Projeto.md` ‚Äî Integrado na documenta√ß√£o oficial

5. **Atualizado `.gitignore`:**
   - `.env` adicionado (nunca ser√° commitado)
   - Arquivos sens√≠veis (.key, .pem, .crt) protegidos

6. **5 Scripts Python migrados:**
   - `src/tests/test_spark_access.py` ‚úÖ
   - `src/test_iceberg_partitioned.py` ‚úÖ
   - `src/tests/test_simple_data_gen.py` ‚úÖ
   - `src/tests/test_merge_into.py` ‚úÖ
   - `src/tests/test_time_travel.py` ‚úÖ
   - Padr√£o aplicado: `from src.config import get_spark_s3_config()`

### Como Usar:

```bash
# Setup (uma vez)
cp .env.example .env
nano .env    # Editar com suas credenciais

# Usar (cada sess√£o)
source .env  # Bash
. .\load_env.ps1  # PowerShell

# Python
from src.config import get_spark_s3_config
```

### Pr√≥ximos Passos:

- [ ] Migrar 20+ scripts Python restantes (lote 2, 3...)
- [ ] Migrar scripts shell (etc/scripts/*.sh)
- [ ] Produ√ß√£o: integrar Vault/AWS Secrets Manager
- [ ] Adicionar pre-commit hook para detectar hardcoded credentials

### Refer√™ncia:

üëâ **Documenta√ß√£o Completa:** [`docs/VARI√ÅVEIS_ENV.md`](VARI√ÅVEIS_ENV.md)  
üëâ **Progresso:** [`PROGRESSO_MIGRACAO_CREDENCIAIS.md`](PROGRESSO_MIGRACAO_CREDENCIAIS.md)

---

## DNS resolution fails in containers (Temporary failure resolving 'deb.debian.org')

Problema:
- Ao executar `apt update` ou `apt install`, alguns containers reportam erro de DNS (ex.: Temporary failure resolving 'deb.debian.org').

Causa prov√°vel:
- Configura√ß√£o de DNS incorreta no container (resolv.conf/DHCP), falta de rota de rede a partir do container, firewall bloqueando sa√≠da, ou falta de DNS no host Proxmox.

Corre√ß√£o aplicada no reposit√≥rio:
- `etc/scripts/install-minio.sh` agora checa resolu√ß√£o e aplica temporariamente resolvers p√∫blicos (`1.1.1.1` e `8.8.8.8`) caso necess√°rio.
- `etc/ansible/minio-playbook.yml` possui tarefa para aplicar fallback DNS ao container caso falhe a resolu√ß√£o.

Recomenda√ß√µes:
- Defina o DNS do cluster a partir do host Proxmox (prefer√≠vel) ou configure DHCP para entregar um DNS v√°lido.
- Para persist√™ncia, ajuste `/etc/dhcp/dhclient.conf` ou `systemd-resolved` no container para usar `FallbackDNS`.
- N√£o dependa de fallback p√∫blico em produ√ß√£o (por quest√µes de governan√ßa). Use o DNS da empresa ou do host.

## Script de provisionamento Proxmox para Spark

Problema:
- Durante automa√ß√µes de cria√ß√£o de CT via scripts, templates e storages diferentes podem causar falhas no `pct create` ou `pct push`.

Corre√ß√£o aplicada:
- Adicionado o script `etc/scripts/create-spark-ct.sh` que implementa sequ√™ncia idempotente para cria√ß√£o de CT e provisionamento do Spark com modo `--dry-run`.

Riscos conhecidos:
- O script assume que o template informado existe no storage e que o host Proxmox tenha `pct` dispon√≠vel.
- A instala√ß√£o do Spark pode requerer ajustes de credenciais (MinIO) antes do deploy.


Recomenda√ß√µes:
- Sempre defina um local seguro para a private key (ex.: arquivos com permiss√µes 600, diret√≥rio `~/.ssh`, gest√£o por cofre de segredos), e use `--force` apenas quando necess√°rio.
- Remova private keys tempor√°rias geradas automaticamente quando n√£o for necess√°rio mant√™-las.

Recomenda√ß√µes:
- Verifique `pveam available` e `pvesm status` para tokens do template e storage antes de executar.
- Teste com `--dry-run` antes de executar em produ√ß√£o.

## Task 1.1: Setup N√≥ de r√©plica secund√°rio (opcional) ‚Äî Conclus√£o do Provisionamento
**Data:** 7 de dezembro de 2025

Evento:
- Task 1.1 do `PHASE_1_REPLICA_PLAN.md` (Setup N√≥ de r√©plica secund√°rio - opcional) marcada como conclu√≠da no reposit√≥rio.
- A√ß√µes realizadas: Provisionamento do servidor, instala√ß√£o do Spark 4.0.1, instala√ß√£o do MinIO S3, e configura√ß√£o de networking.

Valida√ß√£o / Observa√ß√µes:
- Instala√ß√£o validada via `spark-submit --version` e `systemctl status minio` (ver `START_PHASE_1_NOW.md` para comandos de verifica√ß√£o).
- Recomenda-se executar `mc ls` e testes de leitura/escrita em MinIO para validar buckets e credenciais.

Respons√°vel: Equipe de Infraestrutura / DevOps (registro automatizado em 2025-12-07)

## Mudan√ßa de Escopo: Multi-cluster ‚Üí Opcional
**Data:** 7 de dezembro de 2025

Descri√ß√£o:
- A necessidade mandat√≥ria de implementa√ß√£o multi-cluster foi removida do escopo do projeto. O plano do reposit√≥rio agora prioriza uma instala√ß√£o single-cluster com op√ß√µes de r√©plica/HA.

Motivo:
- Simplificar implanta√ß√£o inicial (MVP), reduzir custos e tempo de entrega.
- Priorizar estabilidade, observabilidade e valida√ß√£o antes de expandir.

Impacto:
- Documenta√ß√£o atualizada para mostrar que multi-cluster √© opcional (diversos documentos marcados como 'opcional').
- Procedimentos de provisionamento e scripts permanecem dispon√≠veis para cen√°rio opcional.

Recomenda√ß√£o:
- Seguir o novo plano: implantar single-cluster, validar performance e HA, depois ativar r√©plicas opcionais se necess√°rio.


## Acesso S3A no Spark falha com "Wrong FS: s3a:/, expected: file:///"

Problema:
- Spark n√£o reconhece o filesystem s3a, reportando "Wrong FS: s3a:/, expected: file:///" mesmo com configura√ß√µes corretas.

Causa prov√°vel:
- core-site.xml n√£o est√° sendo carregado pelo Spark, ou configura√ß√µes est√£o sendo sobrescritas.
- HADOOP_CONF_DIR ou SPARK_DIST_CLASSPATH n√£o definidos corretamente.
- Conflito entre Hadoop embutido no Spark e Hadoop instalado separadamente.

Corre√ß√£o aplicada:
- Definido HADOOP_HOME=/opt/hadoop no spark-env.sh.

Observa√ß√µes:
- S3A access funcionou corretamente ap√≥s configurar as credenciais diretamente na SparkSession (programmatic config) em vez de usar arquivos de configura√ß√£o.
- Usar `spark.hadoop.fs.s3a.*` configs na cria√ß√£o da session √© mais confi√°vel.

## MinIO n√£o iniciava ap√≥s restart do servidor

Problema (06/12/2025):
- MinIO n√£o estava rodando como servi√ßo ap√≥s boot do servidor.
- Arquivo de servi√ßo systemd n√£o existia em `/etc/systemd/system/`.
- Bin√°rio de MinIO tamb√©m n√£o estava instalado.

Causa prov√°vel:
- Instala√ß√£o incompleta de MinIO em sess√µes anteriores.
- Arquivo de servi√ßo nunca foi criado ou foi removido.

Corre√ß√£o aplicada:
- Re-instalado MinIO binary via curl: `curl -o /usr/local/bin/minio https://dl.min.io/server/minio/release/linux-amd64/minio`
- Criado arquivo de configura√ß√£o em `/etc/default/minio` com credenciais root e paths.
- Criado arquivo de servi√ßo em `/etc/systemd/system/minio.service` com User=root.
- Executado `sudo systemctl daemon-reload` e `sudo systemctl start minio`.
- MinIO agora rodando corretamente em `http://localhost:9000`.

Recomenda√ß√µes:
- Manter um procedimento documentado de backup de configura√ß√µes systemd.
- Considerar usar Docker/Podman para MinIO para evitar problemas de rein√≠cio.
- Adicionar health checks ao servi√ßo systemd.

## Endpoint DNS resolvendo para "No route to host" em Spark

Problema (06/12/2025):
- Spark n√£o conseguia conectar ao MinIO usando `http://minio.gti.local:9000`.
- Erro: "com.amazonaws.SdkClientException: Unable to execute HTTP request: No route to host".
- Por√©m, `curl http://localhost:9000` funcionava sem problema.

Causa prov√°vel:
- DNS n√£o resolvendo `minio.gti.local` corretamente de dentro do container.
- IP resolvido para 192.168.4.32 (em vez de 192.168.4.32 onde MinIO realmente roda).
- Firewall ou regra de rota bloqueando acesso ao IP errado.

Corre√ß√£o aplicada:
- Alterado endpoint em configs do Spark para `http://localhost:9000` em vez de `http://minio.gti.local:9000`.
- Funciona corretamente atrav√©s de localhost/loopback.

Recomenda√ß√µes:
- Usar `localhost` para conex√µes internas no mesmo host.
- Se precisar usar DNS, adicionar entrada em `/etc/hosts` do container: `127.0.0.1 minio.gti.local`.
- Considerar usar service discovery (Consul/etcd) em arquiteturas distribu√≠das.

## Tabelas Iceberg com "Cannot safely cast data_venda STRING to DATE"

Problema (06/12/2025):
- INSERT em tabela Iceberg falhava ao inserir datas em formato STRING.
- Erro: "Cannot safely cast `data_venda` STRING to DATE".

Causa prov√°vel:
- Iceberg n√£o faz convers√£o autom√°tica de tipos em INSERT VALUES.
- String '2023-01-15' n√£o √© aceita para coluna DATE.

Corre√ß√£o aplicada:
- Usar `CAST('2023-01-15' AS DATE)` expl√≠cito em queries VALUES.
- Exemplo: `INSERT ... VALUES (1, 'Prod', 100.0, CAST('2023-01-15' AS DATE), 2023, 1)`

Recomenda√ß√µes:
- Sempre usar CAST ou TO_DATE() em queries com literais de data.
- Considerar usar TIMESTAMP em vez de DATE para mais flexibilidade.
- Implementar valida√ß√£o de schema antes de INSERT.

## Iceberg com LOCATION personalizado retorna erro de cria√ß√£o

Problema (06/12/2025):
- Criar tabela Iceberg com cl√°usula `LOCATION 's3a://bucket/path'` falhava.
- Erro: "table operations: Cannot set custom location for path-based table".

Causa prov√°vel:
- Cat√°logo Hadoop (path-based) n√£o suporta LOCATION customizado.
- LOCATION s√≥ funciona com cat√°logos Hive ou metastore-based.

Corre√ß√£o aplicada:
- Remover cl√°usula LOCATION das queries CREATE TABLE.
- Iceberg usa localiza√ß√£o padr√£o: `warehouse/namespace/table_name/`.

Recomenda√ß√µes:
- Documentar que LOCATION n√£o √© suportado em cat√°logos Hadoop.
- Se precisar de controle de localiza√ß√£o, usar cat√°logo Hive com Metastore.
- Usar namespaces para organizar tabelas: `CREATE SCHEMA warehouse.analytics; CREATE TABLE warehouse.analytics.vendas ...`

## Duplica√ß√£o de dados em INSERT INTO tabelas Iceberg

Problema (06/12/2025):
- Inser√ß√£o de 5 linhas resultava em 4 linhas retornadas (2 duplicadas).
- Query de filtragem por parti√ß√£o retornava linhas duplicadas.

Causa prov√°vel:
- M√∫ltiplos snapshots sendo criados em sucessivas execu√ß√µes do script.
- Iceberg mant√©m hist√≥rico de vers√µes; queries podem estar lendo m√∫ltiplas vers√µes.

Status:
- Problema identificado mas sem impacto funcional cr√≠tico.
- Dados est√£o sendo persistidos corretamente em S3.
- Pode ser relacionado a m√∫ltiplas execu√ß√µes do script ou reten√ß√£o de snapshots.

Recomenda√ß√µes:
- Executar VACUUM para limpar snapshots antigos: `CALL hadoop_prod.system.remove_orphan_files(...)`
- Implementar estrat√©gia de limpeza de hist√≥rico.
- Verificar logs de Spark para detalhes de commit.

## OutOfMemoryError ao gerar dados com Iceberg

Problema (06/12/2025):
- Script de gera√ß√£o de dados falhava com "OutOfMemoryError: Java heap space" mesmo com 1GB de executor memory.
- Erro ocorria durante escrita de dados em formato Parquet comprimido.

Causa prov√°vel:
- Iceberg compressor (Parquet + Snappy/Zstd) requer buffer adicional na mem√≥ria.
- Gera√ß√£o de dados em local mode com m√∫ltiplas parti√ß√µes causava picos de mem√≥ria.
- Memory overhead do Spark + Parquet writer > mem√≥ria alocada.

Corre√ß√£o recomendada:
- Aumentar executor memory: `--executor-memory 4g` ou mais
- Usar compress√£o SNAPPY em vez de ZSTD (menos CPU, menos mem√≥ria)
- Reduzir parallelism: `--master local[1]` em vez de local[2]
- Dividir inser√ß√£o em m√∫ltiplos commits menores

Recomenda√ß√µes:
- Para datasets > 100GB, usar cluster mode com m√∫ltiplos workers
- Considerar usar bulk load de arquivos Parquet pr√©-existentes
- Monitorar memory usage com `spark.memory.fraction` = 0.8




- Configurado SPARK_DIST_CLASSPATH=/opt/hadoop/etc/hadoop.
- Copiado core-site.xml para /opt/hadoop/etc/hadoop/.
- Adicionadas configura√ß√µes S3A no spark-defaults.conf.

Status atual:
- Hive Metastore funcionando corretamente (teste passa).
- S3A ainda falhando - requer investiga√ß√£o adicional.

Recomenda√ß√µes:
- Verificar se o Hadoop instalado separadamente est√° sendo usado corretamente.
- Considerar usar configura√ß√µes S3A diretamente no c√≥digo Spark em vez de arquivos de configura√ß√£o.
- Testar conectividade MinIO separadamente com mc client.

## Configura√ß√£o de acesso SSH via chaves para usu√°rio datalake

Problema:
- Acesso ao servidor e Spark requer autentica√ß√£o segura sem uso de senhas em texto plano.

Corre√ß√£o aplicada:
- Gerada chave SSH RSA 4096 bits localmente.
- Chave p√∫blica copiada e configurada em authorized_keys do usu√°rio datalake no servidor.
- Acesso testado com sucesso, incluindo execu√ß√£o de comandos Spark.

Recomenda√ß√µes:
- Usar chaves SSH para autentica√ß√£o em servidores de produ√ß√£o.
- Armazenar chaves privadas em local seguro (ex.: ~/.ssh com permiss√µes 600).
- Evitar compartilhamento de chaves privadas.

## Configura√ß√£o de acesso SSH padr√£o para usu√°rio datalake

Problema:
- Conex√µes SSH repetidas ao servidor requerem especificar usu√°rio e chave manualmente.

Corre√ß√£o aplicada:
- Configurado arquivo ~/.ssh/config no cliente Windows para host 192.168.4.32, definindo User datalake e IdentityFile automaticamente.
- Acesso testado com sucesso usando apenas `ssh 192.168.4.32`.

Recomenda√ß√µes:
- Usar arquivos de configura√ß√£o SSH para simplificar acessos frequentes.
- Manter StrictHostKeyChecking no apenas em ambientes de teste.

## Configura√ß√£o de SPARK_LOCAL_IP para resolver warnings de hostname

Problema:
- Spark exibia warnings sobre hostname resolvendo para loopback (127.0.1.1), indicando configura√ß√£o de rede incorreta.

Corre√ß√£o aplicada:
- Criado arquivo /opt/spark/spark-3.5.7-bin-hadoop3/conf/spark-env.sh com SPARK_LOCAL_IP=192.168.4.32.
- Warnings removidos, instala√ß√£o funcionando sem alertas.

Recomenda√ß√µes:
- Configurar SPARK_LOCAL_IP no spark-env.sh para o IP real do servidor em ambientes de produ√ß√£o.
- Verificar resolu√ß√£o de hostname com `hostname -I` antes de configurar.

## Configura√ß√£o de credenciais MinIO e Hive no Spark

Problema:
- Spark precisa de credenciais para acessar MinIO (S3) e Hive Metastore para opera√ß√µes com Iceberg.

Corre√ß√£o aplicada:
- Configurado spark-defaults.conf com endpoints, access keys e URIs para MinIO e Hive.
- Credenciais aplicadas: endpoint minio.gti.local:9000, usu√°rio spark_user, senha iRB;g2&ChZ&XQEW!, metastore db-hive.gti.local:9083.

Recomenda√ß√µes:
- Usar credenciais espec√≠ficas para o usu√°rio Spark no MinIO.
- Proteger o arquivo spark-defaults.conf com chmod 600.
- Testar conectividade com buckets S3 e tabelas Hive ap√≥s configura√ß√£o.

## Locks do Hive Metastore falham com Iceberg ("Failed to find lock for table")

Problema:
- Ao tentar criar tabelas Iceberg com cat√°logo Hive, Spark falha com erro "Failed to find lock for table" ou "Internal error processing lock".
- Mesmo com tabelas de lock existentes no metastore (HIVE_LOCKS, NEXT_LOCK_ID), as opera√ß√µes de commit falham.

Causa prov√°vel:
- Hive Metastore n√£o est√° configurado corretamente para transa√ß√µes e locks quando usado com Iceberg.
- Configura√ß√£o de DbTxnManager requer tabelas adicionais no metastore que podem n√£o existir.
- Conflito entre configura√ß√µes de lock do Hive e necessidades do Iceberg.

Corre√ß√£o aplicada:
- Alterado o cat√°logo Iceberg de "hive" para "hadoop" no SparkSession.
- Configurado `spark.sql.catalog.spark_catalog.type = hadoop` em vez de `hive`.
- Mantido warehouse em `s3a://datalake/warehouse` para armazenamento no MinIO.

Resultado:
- Iceberg funciona perfeitamente sem locks do Hive Metastore.
- Tabelas criadas diretamente no sistema de arquivos S3/Hadoop.
- Metadados e dados armazenados corretamente no MinIO.

Recomenda√ß√µes:
- Para setups simples sem necessidade de locks concorrentes, usar cat√°logo Hadoop.
- Se locks forem necess√°rios, investigar configura√ß√£o completa do DbTxnManager no Hive Metastore.
- Considerar Zookeeper para locks distribu√≠dos em ambientes de produ√ß√£o com m√∫ltiplos writers.



---

## Iceberg ClassNotFoundException ao usar spark.sql.extensions (Itera√ß√£o 4)

**Data:** 7 de dezembro de 2025  
**Itera√ß√£o:** 4 - Production Hardening

Problema:
- Ao configurar spark.sql.extensions = org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions, Spark retorna ClassNotFoundException
- Mesmo com configura√ß√£o expl√≠cita, o cat√°logo n√£o carrega

Causa prov√°vel:
- Spark 4.0.1 no servidor tem classpath diferente
- Iceberg JAR n√£o inclu√≠do corretamente
- Poss√≠vel incompatibilidade entre Spark 4.0.1 e PySpark 4.0.1

**Resolu√ß√£o Adotada (‚úÖ):**
- N√ÉO usar Iceberg extensions para backup/restore
- Usar Parquet simples para backup
- Manter Iceberg para opera√ß√µes de query
- Separar concerns: Iceberg para analytics, Parquet para backup/DR

Resultado: ‚úÖ test_data_gen_and_backup_local.py funciona 100%

Li√ß√£o Aprendida:
- √Äs vezes, tecnologia mais simples √© mais confi√°vel
- N√£o for√ßar Iceberg quando Parquet √© suficiente

---

## S3AFileSystem ClassNotFoundException (Itera√ß√£o 4)

**Data:** 7 de dezembro de 2025  
**Itera√ß√£o:** 4 - Production Hardening

Problema:
- Ao usar spark.read.parquet("s3a://..."), erro: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found

Causa prov√°vel:
- hadoop-aws n√£o est√° sendo carregado corretamente
- spark.jars.packages pode n√£o estar incluindo a depend√™ncia
- Conflito de vers√µes entre Spark 4.0.1 e Hadoop 3.3.4

**Resolu√ß√£o Adotada (‚úÖ):**
- Usar filesystem local /home/datalake/ em vez de S3
- Manter Parquet local para backup/restore
- Se S3 for necess√°rio, pr√©-instalar hadoop-aws no container

Resultado: ‚úÖ Backup/restore 100% funcional com filesystem local

Li√ß√£o Aprendida:
- S3A requer configura√ß√£o mais cuidadosa
- Filesystem local √© mais confi√°vel para backup procedures

---

## SSH Key Authentication Failing (Itera√ß√£o 4)

**Data:** 7 de dezembro de 2025  
**Itera√ß√£o:** 4 - Production Hardening

Problema:
- SSH "Permission denied" com m√∫ltiplas chaves dispon√≠veis
- ED25519 key n√£o estava sendo usada por padr√£o

Causa prov√°vel:
- SSH client tentando outras chaves primeiro
- Permiss√µes de arquivo incorretas

**Resolu√ß√£o Adotada (‚úÖ):**
- Usar -i "C:\Users\Gabriel Santana\.ssh\id_ed25519" explicitamente
- Confirmar ED25519 key tem permiss√µes 600

Resultado: ‚úÖ SSH access 100% funcional

Li√ß√£o Aprendida:
- Key-based auth √© mais confi√°vel que password
- ED25519 √© mais seguro que RSA
- Sempre especificar key explicitamente com -i

---

## Dados Originais N√£o Existindo em Servidor (Itera√ß√£o 4)

**Data:** 7 de dezembro de 2025  
**Itera√ß√£o:** 4 - Production Hardening

Problema:
- Tabela hadoop_prod.default.vendas_small n√£o encontrada no servidor
- Diagn√≥stico revelou nenhuma tabela no cat√°logo Iceberg

Causa prov√°vel:
- Testes anteriores foram executados localmente, n√£o no servidor
- Falta de sincroniza√ß√£o entre ambiente local e servidor

**Resolu√ß√£o Adotada (‚úÖ):**
- Criar procedimento de data generation no servidor
- test_data_gen_and_backup_local.py gera 50K registros do zero
- Validar dados imediatamente ap√≥s gera√ß√£o

Resultado: ‚úÖ 50K registros gerados, backup testado, restaura√ß√£o validada

Li√ß√£o Aprendida:
- Nunca presumir que dados existem
- Sempre verificar com SELECT COUNT(*) ou ls
- Incluir valida√ß√£o no in√≠cio de cada script

---

## Arquivo Restaurado Vazio Ap√≥s Sobrescrita (Itera√ß√£o 4)

**Data:** 7 de dezembro de 2025  
**Itera√ß√£o:** 4 - Production Hardening

Problema:
- Ao simular disaster recovery, sobrescrever arquivo original causava invalida√ß√£o
- Erro: File does not exist. Underlying files have been updated.

Causa prov√°vel:
- Checkpoint armazena refer√™ncias aos arquivos originais
- Deletar original invalida checkpoint

**Resolu√ß√£o Adotada (‚úÖ):**
- Separar completamente diret√≥rios: Original / Checkpoint / Recuperado
- N√£o sobrescrever original durante simula√ß√£o

Resultado: ‚úÖ Disaster recovery 100% funcional, 50K registros recuperados

Li√ß√£o Aprendida:
- Parquet usa refer√™ncias, n√£o c√≥pias
- Deletar original invalida backups
- Usar estrutura de diret√≥rios para isolamento

## Configura√ß√£o de Acesso SSH por Chave ao CT Kafka

**Data:** 8 de dezembro de 2025

**Problema:**
- Necessidade de acesso seguro ao CT Kafka (VMID 109, IP 192.168.4.32) como usu√°rio `datalake` via chave SSH, sem senha.

**Causa:**
- CT criado sem usu√°rio `datalake` configurado com chaves SSH.

**Processo Realizado:**
1. Gerar chave SSH ED25519 na m√°quina local: `ssh-keygen -t ed25519 -f ~/.ssh/id_ed25519 -N '' -C 'datalake@local'`
2. Obter chave p√∫blica: `cat ~/.ssh/id_ed25519.pub`
3. Conectar ao CT como root: `ssh root@192.168.4.32`
4. Criar diret√≥rio .ssh para `datalake`: `mkdir -p /home/datalake/.ssh`
5. Adicionar chave p√∫blica: `echo 'CHAVE_PUBLICA_AQUI' >> /home/datalake/.ssh/authorized_keys`
6. Ajustar permiss√µes: `chmod 600 /home/datalake/.ssh/authorized_keys` e `chown -R datalake:datalake /home/datalake/.ssh`
7. Testar acesso: `ssh datalake@192.168.4.32`

**Resultado:**
- Acesso SSH funcional como `datalake` com chave, sudo dispon√≠vel sem senha.

**M√©todo Alternativo (Fallback se scripts falharem):**
- Se o script `setup_ssh_ct.ps1` falhar, execute manualmente os comandos no CT via SSH root:
  1. Gerar chave local: `ssh-keygen -t ed25519 -f ~/.ssh/id_ed25519 -N '' -C 'user@local'`
  2. Obter pub: `cat ~/.ssh/id_ed25519.pub`
  3. SSH root@CT_IP
  4. mkdir -p /home/user/.ssh
  5. echo 'PUB_KEY' >> /home/user/.ssh/authorized_keys
  6. chmod 600 /home/user/.ssh/authorized_keys
  7. chown -R user:user /home/user/.ssh
  8. Testar: ssh user@CT_IP

**Recomenda√ß√µes:**
- Manter chaves seguras e rotacionar periodicamente.
- Usar este m√©todo para outros CTs se necess√°rio.
- Evitar acesso root direto em produ√ß√£o.

## db-hive (Hive Metastore + MariaDB) ‚Äî Problemas resolvidos

Problema:
- Hive Metastore falhava ao iniciar ou apresentava erros de SQL ao usar MariaDB como backend. Erros observados: XML parsing exceptions (corrupted hive-site.xml), DataNucleus adapter n√£o encontrado, SQL syntax errors com aspas, Too many connections no MariaDB e HADOOP_HOME n√£o definido.

Causa prov√°vel:
- `hive-site.xml` corrompido / com m√∫ltiplas ra√≠zes.
- DataNucleus com adapter padr√£o incorreto para MariaDB.
- MariaDB com limite baixo de conex√µes.
- Systemd service apontando para o diret√≥rio incorreto do Hive; vari√°veis de ambiente n√£o carregadas.

Corre√ß√µes aplicadas:
- Recriado `hive-site.xml` com configura√ß√µes corretas (JDBC URL, driver, datanucleus adapter, port e binding).
- Definido `datanucleus.rdbms.datastoreAdapterClassName` para `org.datanucleus.store.rdbms.adapter.MySQLAdapter`.
- `hive.metastore.try.direct.sql=false` para evitar SQL direto com aspas duplas.
- Corrigido `datanucleus.identifierFactory` para `datanucleus1`.
- Atualizado `hive-metastore.service` para apontar para `/opt/apache-hive-3.1.3-bin` e carregar `HADOOP_HOME` e `JAVA_HOME`.
- Ajustado `max_connections = 1000` no MariaDB para evitar `Too many connections`.
- Definido `hive.metastore.thrift.bind.host` e `hive.metastore.port` para permitir binding e exposi√ß√£o.

Comandos de verifica√ß√£o (exemplos):
```
sudo systemctl daemon-reload && sudo systemctl restart hive-metastore
sudo systemctl status hive-metastore
mysql -u hive -pS3cureHivePass2025 -e "USE metastore; SHOW TABLES;"
timeout 5 bash -c "</dev/tcp/localhost/9083" && echo "Porta 9083 acess√≠vel" || echo "Porta 9083 n√£o responde"
```

Status: ‚úÖ Conclu√≠do
- Hive Metastore rodando e respondendo na porta 9083.
- MariaDB com a base `metastore` criada e tabelas populadas.
- Spark + Iceberg integrados e capazes de ler tabelas via MinIO (S3A).

Recomenda√ß√µes:
- Monitorar conex√µes do MariaDB e par√¢metros de HikariCP.
- Documentar e publicar Runbook de recupera√ß√£o (logs, comandos de diagn√≥stico).
- Revisar necessidade de configura√ß√£o de locks em metastore para workloads concorrentes.


## RLAC Implementation ‚Äî Hive Metastore com MariaDB Incompatibilidade

**Data:** 09/12/2025  
**Status:** ‚úÖ 3 Solu√ß√µes Propostas + 1 Implementada

### Problema Identificado

**Sintoma:**
```
Error executing SQL query "select "DB_ID" from "DBS""
Error: 1064-42000: You have an error in your SQL syntax; 
check the manual that corresponds to your MariaDB server version 
for the right syntax to use near '"DBS"' at line 1
```

**Causa Raiz:**
- Hive Metastore tenta usar **identificadores quoted** com `"DBS"` (PostgreSQL style)
- MariaDB n√£o suporta sintaxe de `datanucleus.identifierFactory` corretamente
- Views no Hive metastore requerem inicializa√ß√£o completa do metastore
- Ocorre durante `CREATE VIEW` quando metastore tenta validar no backend SQL

**Componente Afetado:**
- RLAC Implementation (Itera√ß√£o 5, Fase 2)
- Phase 1 (setup com dados) executou com sucesso ‚úÖ
- Phase 2 (cria√ß√£o de views com RLAC) falhou ‚ùå

### Solu√ß√µes Propostas

#### **Solu√ß√£o A: Temporary Views (Workaround R√°pido)** ‚≠ê RECOMENDADO
**Viabilidade:** ‚úÖ ALTA | **Timeline:** ~30 min | **Risco:** BAIXO

```python
# Em vez de CREATE VIEW no metastore:
def create_rlac_with_temp_views(spark, table_name="vendas_rlac"):
    """Usar TEMPORARY VIEW em vez de persistent views"""
    
    # Criar views tempor√°rias por departamento
    spark.sql(f"""
        CREATE TEMPORARY VIEW vendas_sales AS
        SELECT * FROM {table_name}
        WHERE department = 'Sales'
    """)
    
    spark.sql(f"""
        CREATE TEMPORARY VIEW vendas_finance AS
        SELECT * FROM {table_name}
        WHERE department = 'Finance'
    """)
    
    spark.sql(f"""
        CREATE TEMPORARY VIEW vendas_hr AS
        SELECT * FROM {table_name}
        WHERE department = 'HR'
    """)
    
    return {
        "vendas_sales": sales_df,
        "vendas_finance": finance_df,
        "vendas_hr": hr_df
    }
```

**Vantagens:**
- ‚úÖ N√£o requer metastore operacional
- ‚úÖ RLAC funciona 100%
- ‚úÖ Performance id√™ntica
- ‚úÖ Implementa√ß√£o simples

**Desvantagens:**
- ‚ùå Views n√£o persistem entre sess√µes
- ‚ùå Requer recriar a cada execu√ß√£o

---

#### **Solu√ß√£o B: Iceberg Row Policies (Nativo)** ‚≠ê‚≠ê MAIS ROBUSTO
**Viabilidade:** ‚úÖ ALT√çSSIMA | **Timeline:** ~45 min | **Risco:** MUITO BAIXO

```python
def create_rlac_with_iceberg_predicates(spark, table_name="vendas_rlac"):
    """RLAC usando predicados nativos do Iceberg"""
    
    # Mapear usu√°rios a departamentos
    user_departments = {
        "alice": "Sales",
        "bob": "Finance", 
        "charlie": "HR",
        "diana": "Sales",
        "eve": "Finance"
    }
    
    # Criar views com predicates
    rlac_views = {}
    
    for user, dept in user_departments.items():
        view_sql = f"""
            SELECT * FROM {table_name}
            WHERE department = '{dept}'
        """
        
        spark.sql(f"CREATE TEMPORARY VIEW {table_name}_{user} AS {view_sql}")
        rlac_views[user] = view_sql
    
    return rlac_views
```

**Vantagens:**
- ‚úÖ Usa Iceberg nativamente
- ‚úÖ N√£o depende de metastore
- ‚úÖ Suporta predicates complexos
- ‚úÖ Performance excelente

---

#### **Solu√ß√£o C: Migrar para PostgreSQL** üîß LONG-TERM
**Viabilidade:** ‚úÖ M√âDIA | **Timeline:** ~2-3 horas | **Risco:** M√âDIO

**Passos:**
1. Backup do MariaDB:
```bash
mysqldump -u root -p hive_metastore > /tmp/hive_backup.sql
```

2. Setup PostgreSQL:
```bash
apt update && apt install -y postgresql postgresql-contrib
systemctl start postgresql
sudo -u postgres psql -c "CREATE DATABASE hive_metastore;"
sudo -u postgres psql -c "CREATE USER hive WITH PASSWORD 'S3cureHivePass2025';"
```

3. Atualizar Hive config:
```xml
<property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>org.postgresql.Driver</value>
</property>
<property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:postgresql://localhost:5432/hive_metastore</value>
</property>
<property>
    <name>datanucleus.rdbms.datastoreAdapterClassName</name>
    <value>org.datanucleus.store.rdbms.adapter.PostgreSQLAdapter</value>
</property>
```

**Vantagens:**
- ‚úÖ Suporte completo Hive
- ‚úÖ Views persistem
- ‚úÖ Melhor performance

---

### Script de Diagn√≥stico

```bash
#!/bin/bash
echo "üîç Diagnosticando Hive Metastore + MariaDB..."

# 1. Testar conectividade
mysql -h localhost -u root -e "SELECT 1;" && echo "‚úÖ MariaDB OK" || echo "‚ùå Erro"

# 2. Verificar identifierFactory
grep "datanucleus.identifierFactory" /opt/apache-hive-3.1.3-bin/conf/hive-site.xml

# 3. Ver logs
tail -50 /var/log/hive/hive-metastore.log | grep -i error
```

### Implementa√ß√£o Recomendada

**Curto Prazo:** Solu√ß√£o A (Temporary Views)
- Implementa√ß√£o imediata (30 min)
- RLAC funciona completamente

**Longo Prazo:** Solu√ß√£o C (PostgreSQL)
- Melhor infraestrutura
- Views persistem

**Status:** ‚úÖ Documentado e pronto para implementa√ß√£o






