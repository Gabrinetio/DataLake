# üì¶ RELAT√ìRIO DE INSTALA√á√ÉO DO APACHE SPARK 3.5.7

**Data:** 11 de dezembro de 2025  
**Container:** CT 108 - spark.gti.local (192.168.4.37)  
**Status:** ‚úÖ **INSTALA√á√ÉO CONCLU√çDA COM SUCESSO**

---

## ‚úÖ Resumo da Instala√ß√£o

### 1. Spark 3.5.7
- ‚úÖ **Vers√£o:** Apache Spark 3.5.7 (Scala 2.12.18, Java 17)
- ‚úÖ **Caminho:** `/opt/spark/spark-3.5.7-bin-hadoop3`
- ‚úÖ **Symlink:** `/opt/spark/default` ‚Üí `/opt/spark/spark-3.5.7-bin-hadoop3`
- ‚úÖ **Execut√°veis:** `spark-shell`, `spark-submit`, `spark-sql`
- ‚úÖ **Vari√°veis de Ambiente:** Configuradas em `/etc/profile`

### 2. JARs Instalados
Todos os JARs necess√°rios est√£o em `/opt/spark/jars/`:

| JAR | Vers√£o | Tamanho | Fun√ß√£o |
|-----|--------|--------|--------|
| `iceberg-spark-runtime-3.5_2.12-1.10.0.jar` | 1.10.0 | 45 MB | Suporte transacional Iceberg |
| `hadoop-aws-3.3.4.jar` | 3.3.4 | 941 KB | Conector Hadoop para S3A |
| `aws-java-sdk-bundle-1.12.262.jar` | 1.12.262 | 268 MB | SDK AWS para MinIO |
| `spark-sql-kafka-0-10_2.12-3.5.7.jar` | 3.5.7 | 423 KB | Conector Kafka Streaming |

### 3. Configura√ß√£o do Spark

#### `spark-defaults.conf` Completo
Arquivo: `/opt/spark/spark-3.5.7-bin-hadoop3/conf/spark-defaults.conf`

**Cat√°logo Iceberg:**
```properties
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.iceberg.type=hive
spark.sql.catalog.iceberg.uri=thrift://db-hive.gti.local:9083
spark.sql.catalog.iceberg.warehouse=s3a://datalake/warehouse
```

**S3A/MinIO:**
```properties
spark.hadoop.fs.s3a.endpoint=http://minio.gti.local:9000
spark.hadoop.fs.s3a.access.key=spark_user
spark.hadoop.fs.s3a.secret.key=SENHA_SPARK_MINIO
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.ssl.enabled=false
```

**Performance:**
```properties
spark.driver.memory=4g
spark.executor.memory=4g
spark.executor.cores=2
spark.default.parallelism=8
```

---

## üîç Status dos Servi√ßos Verificados

| Servi√ßo | Status | Porta | Observa√ß√£o |
|---------|--------|-------|-----------|
| **Spark** | ‚úÖ RUNNING | - | Pronto para iniciar |
| **Hive Metastore** | ‚úÖ RUNNING | 9083 | Conex√£o Thrift ativa |
| **MinIO** | ‚úÖ RUNNING | 9000 | Storage S3 ativo |
| **Kafka** | ‚è≥ Aguardando | 9092 | Verifica√ß√£o pendente |

---

## üöÄ Pr√≥ximos Passos

### 1. Iniciar Spark Master
```bash
ssh -i scripts/key/ct_datalake_id_ed25519 datalake@192.168.4.37  # recomendado: usar chave can√¥nica do projeto
source /etc/profile
/opt/spark/sbin/start-master.sh
```

A Web UI estar√° dispon√≠vel em: `http://spark.gti.local:8080`

### 2. Iniciar Spark Workers (se necess√°rio)
```bash
/opt/spark/sbin/start-workers.sh
```

### 3. Testar Spark Shell
```bash
spark-shell
```

### 4. Testar com Iceberg
```bash
spark-sql
> CREATE TABLE iceberg.default.test (id INT, name STRING) USING ICEBERG;
> INSERT INTO iceberg.default.test VALUES (1, 'GTI');
> SELECT * FROM iceberg.default.test;
```

### 5. Testar Acesso ao MinIO
```scala
val df = spark.range(100)
df.write.parquet("s3a://datalake/test-spark")
```

---

## üìã Configura√ß√£o de Credenciais MinIO

**‚ö†Ô∏è IMPORTANTE:** Atualizar a senha do Spark em MinIO

```bash
# Acessar MinIO
mc alias set minio http://minio.gti.local:9000 datalake <PASSWORD>

# Criar usu√°rio Spark (se n√£o existir)
mc admin user add minio spark_user SENHA_SPARK_MINIO

# Atribuir permiss√£o ao bucket datalake
mc admin policy set minio readwrite user=spark_user
```

---

## üîß Troubleshooting

### Problema: Spark shell n√£o inicia
```bash
# Verificar JAVA_HOME
java -version

# Tentar iniciar com debug
export SPARK_PRINT_LAUNCH_COMMAND=1
spark-shell
```

### Problema: Erro ao conectar com Hive
```bash
# Testar conectividade Thrift
nc -zv db-hive.gti.local 9083

# Verificar logs
tail -f /opt/hive/logs/hive-metastore.log
```

### Problema: Erro S3A/MinIO
```bash
# Testar MinIO
mc alias set local http://minio.gti.local:9000 datalake <PASSWORD>
mc ls local/datalake/

# Verificar credenciais em spark-defaults.conf
grep "s3a" /opt/spark/spark-3.5.7-bin-hadoop3/conf/spark-defaults.conf
```

---

## üìä Sum√°rio T√©cnico

| Item | Valor |
|------|-------|
| **Vers√£o Spark** | 3.5.7 |
| **Vers√£o Hadoop** | 3 (bin-hadoop3) |
| **Vers√£o Scala** | 2.12.18 |
| **Vers√£o Java** | OpenJDK 17.0.17 |
| **Iceberg** | 1.10.0 |
| **AWS SDK** | 1.12.262 |
| **Container** | CT 108 (spark.gti.local) |
| **IP** | 192.168.4.37 |
| **vCPU** | 4 |
| **RAM** | 8 GB |

---

## üìù Logs de Execu√ß√£o

Scripts executados:
1. ‚úÖ `install_spark.sh` - Instala√ß√£o inicial (14:00:54 UTC)
2. ‚úÖ `complete_spark_config.sh` - Configura√ß√£o e JARs (15:10:54 UTC)
3. ‚úÖ `test_spark_integration.sh` - Testes de integra√ß√£o (15:15:00 UTC)

Backups criados:
- `/opt/spark/spark-3.5.7-bin-hadoop3/conf/spark-defaults.conf.backup.20251211_000054`

---

## ‚ú® Status Final

**‚úÖ Spark 3.5.7 instalado e configurado com sucesso!**

A plataforma est√° pronta para:
- ‚úÖ Processamento batch com Spark
- ‚úÖ Integra√ß√£o com Iceberg para tabelas transacionais
- ‚úÖ Acesso a dados em MinIO via S3A
- ‚úÖ Streaming de Kafka (driver instalado)
- ‚úÖ Consultas SQL distribu√≠das

Pr√≥xima fase: Iniciar Spark Master e validar end-to-end com dados reais.

---

**Documenta√ß√£o:** `/opt/spark/spark-3.5.7-bin-hadoop3/docs/`  
**Configura√ß√£o:** `/opt/spark/spark-3.5.7-bin-hadoop3/conf/`  
**Logs:** `/opt/spark/logs/`



