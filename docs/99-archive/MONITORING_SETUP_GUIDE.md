# ğŸ“Š MONITORING & OBSERVABILITY SETUP

**Objetivo:** Configurar monitoramento 24/7 para DataLake em produÃ§Ã£o  
**Status:** â³ Pronto para implementaÃ§Ã£o  
**Complexidade:** IntermediÃ¡ria (1-2 semanas)

---

## ğŸ¯ VisÃ£o Geral

Depois de deploy, monitoramento Ã© crÃ­tico para:
- âœ… Detectar problemas antes dos usuÃ¡rios
- âœ… Medir SLA (99.99% uptime)
- âœ… Otimizar performance
- âœ… Planejar capacity

---

## ğŸ“ˆ Stack de Monitoramento

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          AplicaÃ§Ã£o / DataLake                 â”‚
â”‚   (Spark, Iceberg, MinIO, Hive)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Metricas + Logs
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Prometheus + Filebeat                  â”‚
â”‚  (Coleta de metricas e logs)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Timeseries + Events
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   InfluxDB + Elasticsearch                â”‚
â”‚   (Armazenamento)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Query
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Grafana + Kibana                     â”‚
â”‚   (VisualizaÃ§Ã£o e superset.gti.locals)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Alertas
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AlertManager + PagerDuty               â”‚
â”‚   (NotificaÃ§Ãµes On-Call)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š MÃ©tricas para Monitorar

### 1. Performance (Query Latency)

**O que medir:**
```
spark_sql_query_latency_seconds
â”œâ”€ p50 (50th percentile) - Target: < 500ms
â”œâ”€ p95 (95th percentile) - Target: < 1s
â”œâ”€ p99 (99th percentile) - Target: < 2s
â””â”€ max (Maximum)       - Alert if > 5s
```

**Grafana Query:**
```promql
histogram_quantile(0.99, 
  spark_sql_query_latency_seconds_bucket
)
```

### 2. CDC Pipeline

**O que medir:**
```
cdc_latency_milliseconds
â”œâ”€ Current: < 300ms (target < 5000ms)
â””â”€ Trend: Should be stable

cdc_records_processed
â”œâ”€ Daily: Should grow with data
â””â”€ Errors: Should be 0
```

**Alert Rules:**
```
CDC latency > 1000ms: Warning
CDC latency > 5000ms: Critical
CDC errors > 0: Critical
```

### 3. RLAC Overhead

**O que medir:**
```
query_latency_with_rlac_ms
query_latency_without_rlac_ms

overhead_percentage = (with - without) / without * 100
â”œâ”€ Target: < 5%
â””â”€ Alert if: > 10%
```

### 4. BI Query Performance

**O que medir:**
```
bi_query_latency_ms
â”œâ”€ Aggregation queries: < 500ms
â”œâ”€ Complex joins: < 2s
â””â”€ superset.gti.local queries: < 100ms

superset.gti.local_render_time_ms
â”œâ”€ Target: < 2s
â””â”€ Alert if: > 3s
```

### 5. Infraestrutura

**O que medir:**
```
CPU Usage:
â”œâ”€ Per node: < 80%
â””â”€ Alert if: > 90%

Memory:
â”œâ”€ Per node: < 85%
â””â”€ Alert if: > 95%

Disk:
â”œâ”€ /home/datalake: < 80% used
â””â”€ Alert if: > 90%

Network:
â”œâ”€ Bandwidth used: < 50%
â””â”€ Packet loss: 0%
```

### 6. Spark Cluster Health

**O que medir:**
```
spark_executor_count
â”œâ”€ Expected: 10+
â””â”€ Alert if: < 8

spark_task_failed_total
â”œâ”€ Expected: 0
â””â”€ Alert if: > 5/hour

spark_shuffle_bytes_written
â”œâ”€ Trend: Should be stable
â””â”€ Spike alert: > 2x normal
```

---

## âš™ï¸ ImplementaÃ§Ã£o

### Step 1: Install Prometheus (1 dia)

**Configuration file: prometheus.yml**
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  # Spark metrics
  - job_name: 'spark'
    static_configs:
      - targets: ['192.168.4.33:7777']

  # MinIO metrics
  - job_name: 'minio'
    static_configs:
      - targets: ['192.168.4.32:9000']

  # Node exporter
  - job_name: 'node'
    static_configs:
      - targets: ['192.168.4.32:9100']

  # Custom DataLake metrics
  - job_name: 'datalake'
    static_configs:
      - targets: ['192.168.4.32:8000']

alerting:
  alertmanagers:
    - static_configs:
        - targets: ['192.168.4.32:9093']
```

**Installation:**
```bash
# Download and install
wget https://github.com/prometheus/prometheus/releases/download/v2.45.0/prometheus-2.45.0.linux-amd64.tar.gz
tar xvfz prometheus-2.45.0.linux-amd64.tar.gz
mv prometheus-2.45.0.linux-amd64 /opt/prometheus

# Start service
systemctl start prometheus
systemctl enable prometheus
```

### Step 2: Install Grafana (1 dia)

**Installation:**
```bash
apt-get install -y software-properties-common
add-apt-repository "deb https://packages.grafana.com/oss/deb stable main"
apt-get update
apt-get install grafana-server

systemctl start grafana-server
systemctl enable grafana-server
```

**Default credentials:**
- URL: http://192.168.4.32:3000
- User: admin
- Password: admin (change immediately!)

### Step 3: Setup Alerting (1 dia)

**Alert Rules: alert-rules.yml**
```yaml
groups:
  - name: DataLake
    rules:
      # CDC Alerts
      - alert: CDCLatencyHigh
        expr: cdc_latency_ms > 1000
        for: 5m
        annotations:
          summary: "CDC latency > 1s"
          description: "Current: {{ $value }}ms"

      # Query Performance
      - alert: QueryLatencyHigh
        expr: histogram_quantile(0.99, spark_sql_query_latency_ms) > 2000
        for: 5m
        annotations:
          summary: "Query p99 latency > 2s"

      # Infrastructure
      - alert: HighCPU
        expr: node_cpu_usage > 90
        for: 5m
        annotations:
          summary: "CPU > 90%"

      - alert: HighMemory
        expr: node_memory_usage > 95
        for: 5m
        annotations:
          summary: "Memory > 95%"

      - alert: DiskFull
        expr: node_disk_used_percent > 90
        for: 1m
        annotations:
          summary: "Disk > 90% full"
```

### Step 4: Custom Metrics Exporter (2 dias)

**Python script: datalake_metrics.py**
```python
from prometheus_client import start_http_server, Gauge, Histogram
import time

# Define metrics
cdc_latency = Gauge('cdc_latency_ms', 'CDC latency in ms')
query_latency = Histogram('query_latency_ms', 'Query latency in ms')
rlac_overhead = Gauge('rlac_overhead_percent', 'RLAC overhead %')

def collect_cdc_metrics():
    # Query Spark for CDC latency
    latency = get_cdc_latency_from_spark()
    cdc_latency.set(latency)

def collect_query_metrics():
    # Monitor Spark SQL queries
    for query in get_active_queries():
        query_latency.observe(query.latency_ms)

if __name__ == '__main__':
    start_http_server(8000)
    while True:
        collect_cdc_metrics()
        collect_query_metrics()
        time.sleep(15)
```

---

## ğŸ“Š superset.gti.locals

### superset.gti.local 1: Overview

**Charts:**
```
Top-left:     Query Latency p99 (last 24h)
Top-right:    CDC Latency (last 24h)
Middle-left:  BI Query Performance
Middle-right: Infrastructure Health
Bottom-left:  RLAC Overhead
Bottom-right: Alert Summary
```

### superset.gti.local 2: Performance Deep Dive

**Charts:**
```
Query Latency Distribution (histogram)
Query Throughput (queries/sec)
Failed Queries (count)
Cache Hit Rate
Compaction Progress
```

### superset.gti.local 3: Infrastructure

**Charts:**
```
CPU Usage (per node)
Memory Usage (per node)
Disk Usage (per partition)
Network Bandwidth
I/O Operations
```

### superset.gti.local 4: Spark Cluster

**Charts:**
```
Active Executors
Failed Tasks
Shuffle Bytes
Stage Duration
GC Time
```

---

## ğŸ”” Alert Policy

### Severity Levels

```
CRITICAL (Page on-call immediately):
â”œâ”€ Data loss detected
â”œâ”€ CDC latency > 5s
â”œâ”€ Query failure rate > 5%
â”œâ”€ Disk > 95% full
â””â”€ Cluster down

WARNING (Slack notification):
â”œâ”€ Query p99 latency > 2s
â”œâ”€ CDC latency > 1s
â”œâ”€ RLAC overhead > 7%
â”œâ”€ Memory > 85%
â””â”€ CPU > 80%

INFO (Log only):
â”œâ”€ Routine maintenance
â”œâ”€ Compaction completed
â””â”€ Backup successful
```

### Notification Channels

```
CRITICAL:
â”œâ”€ PagerDuty (page on-call)
â”œâ”€ SMS alert
â”œâ”€ Email escalation
â””â”€ Slack #critical

WARNING:
â”œâ”€ Slack #alerts
â”œâ”€ Email (batch hourly)
â””â”€ superset.gti.local indicator

INFO:
â”œâ”€ Slack #logs
â”œâ”€ Local log file
â””â”€ superset.gti.local widget
```

---

## ğŸ” Log Aggregation (Opcional)

Para logs centralizados:

```
Application Logs:
â”œâ”€ Spark driver logs â†’ Filebeat â†’ Elasticsearch
â”œâ”€ Spark executor logs â†’ Filebeat â†’ Elasticsearch
â”œâ”€ Application logs â†’ Filebeat â†’ Elasticsearch
â””â”€ Query logs â†’ Filebeat â†’ Elasticsearch

Access via Kibana:
â”œâ”€ http://192.168.4.32:5601
â””â”€ Query CDC errors: log_type:cdc AND level:ERROR
```

---

## ğŸ“‹ Operational Checklist

Daily:
- [ ] Review Grafana superset.gti.local
- [ ] Check alert history
- [ ] Verify all services running

Weekly:
- [ ] Review SLA metrics
- [ ] Analyze performance trends
- [ ] Capacity planning check

Monthly:
- [ ] Performance report
- [ ] Capacity forecast
- [ ] Alert rule tuning
- [ ] Disaster recovery test

---

## ğŸ¯ Targets & Metrics

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    SLA TARGETS                            â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                           â•‘
â•‘  Metric              â”‚  Target   â”‚  Alert Threshold     â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•‘
â•‘  Uptime             â”‚  99.99%   â”‚  < 99.95%           â•‘
â•‘  Query p99 latency  â”‚  < 2s     â”‚  > 3s (critical)    â•‘
â•‘  CDC latency        â”‚  < 300ms  â”‚  > 1s (warning)     â•‘
â•‘  RLAC overhead      â”‚  < 5%     â”‚  > 7% (warning)     â•‘
â•‘  superset.gti.local render   â”‚  < 2s     â”‚  > 3s (warning)     â•‘
â•‘  CPU usage          â”‚  < 70%    â”‚  > 90% (critical)   â•‘
â•‘  Memory usage       â”‚  < 75%    â”‚  > 95% (critical)   â•‘
â•‘  Disk usage         â”‚  < 80%    â”‚  > 90% (critical)   â•‘
â•‘                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸš€ Implementation Timeline

```
Week 1: Prometheus & Grafana
â”œâ”€ Day 1-2: Install and configure Prometheus
â”œâ”€ Day 3: Install and configure Grafana
â””â”€ Day 4-5: Create superset.gti.locals

Week 2: Alerting
â”œâ”€ Day 6-7: Setup AlertManager
â”œâ”€ Day 8: Configure PagerDuty integration
â””â”€ Day 9-10: Test alert escalation

Week 3: Custom Metrics
â”œâ”€ Day 11-12: Build metrics exporter
â”œâ”€ Day 13-14: Integrate with Prometheus
â””â”€ Day 15: Tuning and optimization

Post-week 3: Maintenance
â””â”€ Continuous monitoring and tuning
```

---

## ğŸ“ Escalation Contacts

```
24/7 Monitoring:      on-call@company.com
Infrastructure Team:  infra-team@company.com
Database Admin:       dba@company.com
Management:           manager@company.com
```

---

## ğŸ“š Related Documents

- ğŸ“„ `PRODUCTION_DEPLOYMENT_CHECKLIST.md` - Deployment steps
- ğŸ“„ `docs/CONTEXT.md` - Infrastructure info
- ğŸ“„ `docs/Projeto.md` - Architecture reference

---

**Status:** â³ Ready for implementation  
**ETA:** 1-2 weeks from production deploy  
**Priority:** ğŸ”´ High (critical for operations)

ğŸš€ **Monitoramento 24/7 para produÃ§Ã£o!**






