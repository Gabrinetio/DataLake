# üöÄ PHASE 1 - EXECUTION PLAN
## Production Deployment - Week 1-2 (8-21 december)

**Data:** 7 de dezembro de 2025  
**Status:** ‚è≥ Iniciando execu√ß√£o AGORA  
**Meta:** MVP LIVE em Produ√ß√£o

---

## üìã CHECKLIST SIMPLIFICADO - COMECE AQUI

### ‚úÖ Pr√©-requisitos (Validar AGORA)

```powershell
# 1. Verificar conex√£o com servidor
ssh -i C:\Users\Gabriel Santana\.ssh\id_ed25519 datalake@192.168.4.37 "echo 'Connection OK'"

# 2. Verificar Spark est√° rodando
ssh -i C:\Users\Gabriel Santana\.ssh\id_ed25519 datalake@192.168.4.37 "spark-submit --version"

# 3. Verificar MinIO est√° rodando
ssh -i C:\Users\Gabriel Santana\.ssh\id_ed25519 datalake@192.168.4.37 "pgrep -f minio"

# 4. Verificar espa√ßo em disco
ssh -i C:\Users\Gabriel Santana\.ssh\id_ed25519 datalake@192.168.4.37 "df -h /home/datalake"
```

**Status:**
- [ ] Connection OK
- [ ] Spark running
- [ ] MinIO running
- [ ] Disk space OK (>100GB free)

---

### üì§ STEP 1: Upload Scripts (30 min)

Copiar os 3 scripts Iter5 para servidor:

```powershell
# Copiar CDC pipeline
scp -i C:\Users\Gabriel Santana\.ssh\id_ed25519 `
    src\tests\test_cdc_pipeline.py `
    datalake@192.168.4.37:/home/datalake/

# Copiar RLAC implementation
scp -i C:\Users\Gabriel Santana\.ssh\id_ed25519 `
    src\tests\test_rlac_implementation.py `
    datalake@192.168.4.37:/home/datalake/

# Copiar BI integration
scp -i C:\Users\Gabriel Santana\.ssh\id_ed25519 `
    src\tests\test_bi_integration.py `
    datalake@192.168.4.37:/home/datalake/

# Verificar upload
ssh -i C:\Users\Gabriel Santana\.ssh\id_ed25519 datalake@192.168.4.37 "ls -lh *.py"
```

**Status:**
- [ ] CDC uploaded
- [ ] RLAC uploaded
- [ ] BI uploaded
- [ ] All files verified

---

### ‚öôÔ∏è STEP 2: Execute Tests (1h cada)

Executar os 3 testes em produ√ß√£o:

```bash
# TEST 1: CDC Pipeline (10-15 min)
cd /home/datalake
spark-submit --master spark://192.168.4.37:7077 \
  --packages org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.10.0 \
  --driver-memory 4G \
  --executor-memory 4G \
  test_cdc_pipeline.py

# TEST 2: RLAC Implementation (10-15 min)
spark-submit --master spark://192.168.4.37:7077 \
  --packages org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.10.0 \
  --driver-memory 4G \
  --executor-memory 4G \
  test_rlac_implementation.py

# TEST 3: BI Integration (10-15 min)
spark-submit --master spark://192.168.4.37:7077 \
  --packages org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.10.0 \
  --driver-memory 4G \
  --executor-memory 4G \
  test_bi_integration.py
```

**Expected Results:**
- [ ] CDC latency: ~245ms (‚úì < 5min target)
- [ ] RLAC overhead: ~4.51% (‚úì < 5% target)
- [ ] BI max query: ~567ms (‚úì < 30s target)

---

### üìä STEP 3: Collect Results (30 min)

Depois de executar os 3 testes, coletar os JSONs:

```bash
# Copy result files back
scp -i /keys/id_ed25519 datalake@192.168.4.37:/home/datalake/*_results.json .

# Verify results
ls -lh *_results.json
cat cdc_pipeline_results.json | jq .
cat rlac_implementation_results.json | jq .
cat bi_integration_results.json | jq .
```

**Status:**
- [ ] CDC results collected
- [ ] RLAC results collected
- [ ] BI results collected
- [ ] All validations passed

---

### ‚úÖ STEP 4: Validate Production Data (30 min)

Validar que dados est√£o corretos em produ√ß√£o:

```bash
# Check Hive tables
ssh -i /keys/id_ed25519 datalake@192.168.4.32 'hive -e "SHOW TABLES;"'

# Check MinIO buckets
ssh -i /keys/id_ed25519 datalake@192.168.4.37 'mc ls datalake/raw'

# Check record counts
ssh -i /keys/id_ed25519 datalake@192.168.4.37 \
  'spark-sql -e "SELECT COUNT(*) FROM iceberg_table;"'

# Verify data integrity
ssh -i /keys/id_ed25519 datalake@192.168.4.37 \
  'spark-sql -e "SELECT * FROM iceberg_table LIMIT 5;"'
```

**Status:**
- [ ] Hive tables accessible
- [ ] MinIO buckets readable
- [ ] Record counts match expected
- [ ] Data sample looks correct

---

### üéØ STEP 5: Go/No-Go Decision (30 min)

Reviewar resultados e fazer decis√£o final:

**Check Results:**
```
CDC Pipeline:
‚úì Latency: 245.67ms (49x melhor que target)
‚úì Correctness: 100%
‚úì Latency Stability: ¬±15ms

RLAC Implementation:
‚úì Overhead: 4.51% (within target)
‚úì Enforcement: 100%
‚úì Performance: Acceptable

BI Integration:
‚úì Max Query: 567.3ms (53x melhor que target)
‚úì superset.gti.local: 1.515s (within SLA)
‚úì Responsiveness: Good
```

**Sign-off Checklist:**
- [ ] All 3 features working
- [ ] All performance targets MET
- [ ] Data integrity validated
- [ ] Team sign-off obtained
- [ ] Rollback plan ready

**GO/NO-GO DECISION:**
- [ ] **GO** - Proceed to Phase 2
- [ ] **NO-GO** - Rollback (ver se√ß√£o abaixo)

---

## ‚öôÔ∏è ROLLBACK (se necess√°rio)

Se algum teste falhar, rollback √© r√°pido:

```bash
# 1. Stop Spark jobs
ssh -i /keys/id_ed25519 datalake@192.168.4.37 'pkill -f spark-submit'

# 2. Restore from backup
ssh -i /keys/id_ed25519 datalake@192.168.4.37 \
  'cp -r /home/datalake/backups/pre_iter5/* /home/datalake/warehouse/'

# 3. Restart services
ssh -i /keys/id_ed25519 datalake@192.168.4.37 \
  'systemctl restart spark-master spark-worker hive-metastore'

# 4. Verify restored state
ssh -i /keys/id_ed25519 datalake@192.168.4.37 \
  'spark-sql -e "SELECT COUNT(*) FROM iceberg_table;"'
```

**Rollback Duration:** ~5-10 minutos

---

## üìÖ TIMELINE

```
Hoje (7 dez):
‚îú‚îÄ 14:00 - Validar pr√©-requisitos
‚îú‚îÄ 14:30 - Upload scripts (30 min)
‚îî‚îÄ 15:00 - Ready for testing

Amanh√£ (8 dez) - TEST DAY:
‚îú‚îÄ 09:00 - CDC Pipeline test (15 min)
‚îú‚îÄ 09:30 - RLAC test (15 min)
‚îú‚îÄ 10:00 - BI test (15 min)
‚îú‚îÄ 10:30 - Results collection (15 min)
‚îú‚îÄ 11:00 - Data validation (30 min)
‚îî‚îÄ 11:30 - GO/NO-GO decision

Semana seguinte:
‚îú‚îÄ Deploy para produ√ß√£o oficial
‚îú‚îÄ 24/7 monitoring start
‚îî‚îÄ PHASE 2: Team Training
```

---

## üéØ Success Criteria

‚úÖ **MVP LIVE** when:
1. All 3 Iter5 features passing production tests
2. All performance targets exceeded
3. Data integrity validated
4. Team signed off
5. Rollback plan confirmed

**Current Status:** 90% ‚Üí 100% (production ready!)

---

## ‚ö° Quick Links

- [Full Deployment Checklist](../20-operations/checklists/PRODUCTION_DEPLOYMENT_CHECKLIST.md)
- [Iter 5 Results](../ARQUIVO/ITERATION_5_RESULTS.md)
- [Team Handoff](TEAM_HANDOFF_DOCUMENTATION.md)
- [Executive Summary](../00-overview/EXECUTIVE_SUMMARY.md)

---

**Let's go! üöÄ**




