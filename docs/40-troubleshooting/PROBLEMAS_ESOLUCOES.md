# Problemas e Solu√ß√µes

## ‚úÖ Atualiza√ß√£o de Credenciais nos CTs via Vault
**Data:** 20 de dezembro de 2025
**Status:** ‚úÖ Resolvido

**Sintoma:** Necessidade de atualizar credenciais de produ√ß√£o (senhas, tokens, chaves) nos containers LXC ap√≥s migra√ß√£o para HashiCorp Vault, com complica√ß√µes de acesso SSH via Windows.

**Causa Raiz:**
- Credenciais hardcoded nos servi√ßos substitu√≠das por refer√™ncias ao Vault KV v2
- Limita√ß√µes do OpenSSH Windows com senhas e necessidade de sshpass
- Complexidade de escaping de caracteres especiais em comandos SSH aninhados

**Solu√ß√£o Aplicada:**
1. **Instala√ß√£o de ferramentas no WSL Ubuntu-24.04:**
   - `sshpass` para autentica√ß√£o SSH com senha
   - `jq` para processamento JSON das respostas do Vault

2. **Cria√ß√£o de script bash (`update_ct_credentials_wsl.sh`) que:**
   - L√™ credenciais do Vault via API REST
   - Gera scripts tempor√°rios nos CTs via SSH + Proxmox pct
   - Executa atualiza√ß√µes remotas e limpa arquivos tempor√°rios

3. **Wrapper PowerShell (`update_ct_credentials_wsl.ps1`) que:**
   - Valida vari√°veis de ambiente (VAULT_ADDR, VAULT_TOKEN, PROXMOX_PASSWORD)
   - Executa script bash no WSL com vari√°veis inline
   - Fornece feedback visual e tratamento de erros

4. **Atualiza√ß√£o bem-sucedida nos 5 CTs:**
   - CT 116 (Airflow): senha admin atualizada
   - CT 108 (Spark): token de autentica√ß√£o atualizado
   - CT 109 (Kafka): senha SASL atualizada
   - CT 107 (MinIO): access_key/secret_key atualizados
   - CT 117 (Hive): senha PostgreSQL atualizada

**Comandos Executados:**
```bash
# Exemplo do comando SSH gerado:
sshpass -p 'SENHA_PROXMOX' ssh -o StrictHostKeyChecking=no root@192.168.4.25 \
  "pct exec 116 -- su - datalake -c \"
    cat > /tmp/update_cred_PID.sh << 'EOF'
# Script de atualiza√ß√£o aqui
EOF
    chmod +x /tmp/update_cred_PID.sh && /tmp/update_cred_PID.sh && rm /tmp/update_cred_PID.sh
  \\""
```

**Verifica√ß√£o:**
- Todos os CTs reportaram "atualizado com sucesso"
- Credenciais validadas no Vault antes da atualiza√ß√£o
- Scripts tempor√°rios criados e removidos automaticamente
- Sem exposi√ß√£o de credenciais em logs ou arquivos persistentes

**Verifica√ß√£o:**
- Todos os CTs reportaram "atualizado com sucesso"
- Credenciais validadas no Vault antes da atualiza√ß√£o
- Scripts tempor√°rios criados e removidos automaticamente
- Sem exposi√ß√£o de credenciais em logs ou arquivos persistentes

**A√ß√µes Futuras Recomendadas:**
- Implementar rota√ß√£o autom√°tica de credenciais via Vault Agent
- Criar health checks para validar conectividade dos servi√ßos com novas credenciais
- Documentar procedimento de rollback em caso de falha

## ‚úÖ Upload de Chave SSH Can√¥nica para Vault
**Data:** 20 de dezembro de 2025
**Status:** ‚úÖ Resolvido

**Sintoma:** Chave SSH can√¥nica (ct_datalake_id_ed25519) armazenada localmente, necessitando armazenamento seguro no Vault para acesso centralizado e seguro.

**Causa Raiz:**
- Chave privada SSH cr√≠tica para acesso aos CTs
- Necessidade de armazenamento seguro fora do controle de vers√£o
- Requisito de acesso centralizado para automa√ß√£o

**Solu√ß√£o Aplicada:**
1. **Cria√ß√£o de script PowerShell (`upload_ssh_key_to_vault.ps1`):**
   - L√™ chave privada e p√∫blica dos arquivos locais
   - Valida conectividade com Vault
   - Faz upload via API REST KV v2
   - Suporte a DryRun para valida√ß√£o

2. **Estrutura de Armazenamento:**
   - Path: `secret/ssh/canonical`
   - Dados: `private_key` e `public_key`
   - Formato: JSON compat√≠vel com Vault KV v2

3. **Execu√ß√£o bem-sucedida:**
   - Chave privada ED25519 armazenada
   - Chave p√∫blica inclu√≠da para refer√™ncia
   - Valida√ß√£o via API REST confirmada

**Comandos Executados:**
```powershell
# Upload da chave
.\scripts\upload_ssh_key_to_vault.ps1 -KeyPath .\scripts\key\ct_datalake_id_ed25519

# Verifica√ß√£o
curl -H "X-Vault-Token: $TOKEN" "$VAULT_ADDR/v1/secret/data/ssh/canonical"
```

**Verifica√ß√£o:**
- Upload retornou sucesso (HTTP 200)
- Chave recuper√°vel via API: `secret/ssh/canonical`
- Formato JSON v√°lido com campos `private_key` e `public_key`
- Sem exposi√ß√£o da chave privada em logs

**A√ß√µes Futuras Recomendadas:**
- Implementar recupera√ß√£o autom√°tica da chave via scripts
- Configurar rota√ß√£o peri√≥dica da chave SSH
- Documentar uso da chave do Vault em procedimentos de automa√ß√£o

## HiveServer2 - ClassCastException e permiss√£o /tmp/hive (db-hive)
**Data:** 16 de dezembro de 2025  
**Status:** ‚úÖ Resolvido

**Sintoma:** HiveServer2 encerrava na inicializa√ß√£o com `ClassCastException: AppClassLoader cannot be cast to URLClassLoader` ao aplicar a pol√≠tica de autoriza√ß√£o e, ap√≥s corrigir Java, falhava por permiss√£o insuficiente em `/tmp/hive` no HDFS.

**Causa Raiz:**
- Java 17 (classe AppClassLoader modular) incompat√≠vel com o classloader esperado pelo Hive 3.1.3 durante a aplica√ß√£o da pol√≠tica de autoriza√ß√£o.
- Diret√≥rio `/tmp/hive` no HDFS sem permiss√£o de escrita para o usu√°rio `hive`.

**Solu√ß√£o Aplicada:**
1. Instala√ß√£o manual do Temurin JDK 8 em `/opt/java/temurin-8` (tar.gz Adoptium).
2. Cria√ß√£o de script de start `/tmp/start_hs2.sh` exportando `JAVA_HOME=/opt/java/temurin-8`, `HADOOP_HOME=/opt/hadoop`, `HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop`, `HIVE_HOME=/opt/hive`, `HIVE_CONF_DIR=$HIVE_HOME/conf` e executando:
    - `nohup /opt/hive/bin/hiveserver2 --hiveconf hive.server2.authentication=NOSASL --hiveconf hive.server2.thrift.port=10000 --hiveconf hive.root.logger=INFO,DRFA > /opt/hive/logs/hiveserver2.out 2>&1 &`
3. Ajuste de permiss√µes no HDFS para o diret√≥rio tempor√°rio:
    - `sudo -u hive JAVA_HOME=/opt/java/temurin-8 /opt/hadoop/bin/hdfs dfs -chmod 777 /tmp/hive`
4. Rein√≠cio do HiveServer2 usando o script acima (executado como usu√°rio `hive`).

**Verifica√ß√£o:**
- Processo ativo: `ps -ef | grep hiveserver2 | grep -v grep` mostra Java em `/opt/java/temurin-8`.
- Porta aberta: `ss -tlnp | grep 10000` retorna `LISTEN` em `0.0.0.0:10000`.
- Logs em `/opt/hive/logs/hiveserver2.out` sem novas exce√ß√µes ap√≥s o `chmod` do `/tmp/hive`.
- Diret√≥rio `/tmp/hive` no HDFS com permiss√£o `drwxrwxrwx`.

**A√ß√µes Futuras Recomendadas:**
- Criar unit systemd para `/tmp/start_hs2.sh` garantindo `JAVA_HOME=/opt/java/temurin-8` e depend√™ncia do metastore.
- Documentar a URL de conex√£o para Superset: `thrift://db-hive.gti.local:10000/default` com `NOSASL`.

## Superset + PostgreSQL (CT 115)

### ‚úÖ PostgreSQL instalado e configurado
**Data:** 12 de dezembro de 2025  
**Status:** RESOLVIDO

**Problema:**
Superset necessitava de um banco de dados para armazenar metadados (usu√°rios, dashboards, datasets, etc.). A solu√ß√£o anterior usava SQLite, que n√£o √© recomendado para produ√ß√£o.

**Solu√ß√£o Implementada:**
1. Instala√ß√£o do PostgreSQL 15 no CT 115 (superset)
2. Servi√ßo PostgreSQL iniciado e habilitado
3. Driver `psycopg2-binary` instalado no venv do Superset
4. Configura√ß√£o em `/opt/superset/superset_config.py`:
   ```python
   SECRET_KEY = "80/oGMZg02v74/xMojMzugowMKlkJyOnmXmULDeoHkbVRWgo9i1WEX/l"
   SQLALCHEMY_DATABASE_URI = "postgresql://postgres@localhost/postgres"
   ```

**Verifica√ß√£o:**
```bash
# Status do servi√ßo
pct exec 115 -- systemctl status postgresql

# Processos PostgreSQL rodando
pct exec 115 -- ps aux | grep postgres

# Verificar se a porta 5432 est√° aberta
pct exec 115 -- netstat -tlnp | grep 5432
```

**Pr√≥ximos Passos:**
- Reiniciar Superset para aplicar a configura√ß√£o
- Executar `superset db upgrade` para criar tabelas
- Testar acesso a dashboards e datasets via interface web ‚Äî Documenta√ß√£o de Troubleshooting

## Gitea SSH via Proxmox (CT 118) - RESOLVIDO

**Data:** 12 de dezembro de 2025  
**Status:** ‚úÖ RESOLVIDO (Solu√ß√£o Definitiva)

### Problema Original:
SSH direto para CT 118 (192.168.4.26) resultava em "Connection timed out".

### Causa Raiz (Identificada ap√≥s diagn√≥stico):
- ‚ùå N√ÉO era firewall/roteamento Proxmox
- ‚ùå N√ÉO era ip_forward desabilitado
- ‚úÖ Era **limita√ß√£o de rede/isolamento do container LXC**
- Containers LXC em Proxmox t√™m restri√ß√µes de roteamento para m√°quinas externas

### Solu√ß√£o Final Adotada:
**Usar `pct exec` via Proxmox com Autentica√ß√£o por Senha - Simples, Seguro, Confi√°vel**

1. ‚úÖ Script wrapper: `scripts/ct118_access.ps1`
   ```powershell
   # Definir vari√°vel de ambiente com senha
   $env:PROXMOX_PASSWORD = 'sua_senha_proxmox'
   
   # Ou passar como par√¢metro
   .\scripts\ct118_access.ps1 -Command "whoami" -User "datalake" -ProxmoxPassword "sua_senha"
   ```

2. ‚úÖ SSH via Proxmox direto (com senha):
   ```bash
   # Usando sshpass para automa√ß√£o
   sshpass -p 'sua_senha' ssh -o StrictHostKeyChecking=no root@192.168.4.25 'pct exec 118 -- su - datalake -c "comando"'
   ```

3. ‚úÖ Gitea web UI: `http://192.168.4.26:3000` (funciona normalmente)

### Autentica√ß√£o Proxmox:
- ‚ùå **N√ÉO** usar chaves SSH (removido)
- ‚úÖ **SIM** usar autentica√ß√£o por senha
- Motivo: Simplicidade, compatibilidade com scripts, sem gerenciamento de chaves

### Por que √© a Melhor Solu√ß√£o:
- ‚úÖ Seguro (autentica√ß√£o Proxmox obrigat√≥ria via senha)
- ‚úÖ Simples (sem port forwarding complexo)
- ‚úÖ Confi√°vel (usa mecanismo nativo do Proxmox)
- ‚úÖ Sem overhead de DNAT/iptables
- ‚úÖ Padr√£o da ind√∫stria para LXC
- ‚úÖ Sem necessidade de gerenciar chaves SSH

### Status Final:
- Gitea service: ‚úÖ Ativo
- MariaDB: ‚úÖ Ativo
- HTTP 3000: ‚úÖ Acess√≠vel
- SSH direto: ‚ùå N√£o necess√°rio (use pct exec)
- SSH via wrapper com senha: ‚úÖ Funciona perfeitamente
- Proxmox acesso: ‚úÖ Apenas porta 22, autentica√ß√£o por senha

### Li√ß√µes Aprendidas:
1. SSH direto a LXC containers em Proxmox pode ter limita√ß√µes de roteamento
2. `pct exec` √© a forma correta de acessar containers
3. Port forwarding/DNAT adiciona complexidade desnecess√°ria
4. Solu√ß√£o simples √© sempre melhor

**√öltima Atualiza√ß√£o:** 12/12/2025  
**Total de Solu√ß√µes:** 14+

## Spark Workers ‚Äî SPARK_WORKER_OPTS com -Xmx (22 de dezembro de 2025)

**Data:** 22 de dezembro de 2025  
**Status:** ‚úÖ Resolvido

**Problema:**
- Workers do Spark n√£o conseguiam iniciar; logs mostravam "SPARK_WORKER_OPTS is not allowed to specify max heap(Xmx)".

**Causa:**
- `SPARK_WORKER_OPTS` continha `-Xmx...` (defini√ß√£o de heap) que n√£o √© permitido para os workers, impedindo a inicializa√ß√£o do processo worker.

**Solu√ß√£o Aplicada:**
1. Criei backup de `/opt/spark/spark-3.5.7-bin-hadoop3/conf/spark-env.sh`.
2. Removi ocorr√™ncias `-Xmx...` de `SPARK_WORKER_OPTS` e garanti flags permitidas (`-XX:+UseG1GC`, `-XX:MaxGCPauseMillis=200`).
3. Iniciei worker(s) (`/opt/spark/.../sbin/start-worker.sh`) e verifiquei o processo ativo.
4. Re-submeti job de sanity (`SparkPi`) ao cluster; job concluiu com sucesso (Pi ~3.14).

**Comandos/Opera√ß√µes Executadas:**
- `cp /opt/spark/.../conf/spark-env.sh /opt/spark/.../conf/spark-env.sh.bak.<ts>`
- script de limpeza que remove `-Xmx` e adiciona flags permitidas
- `/opt/spark/.../sbin/start-worker.sh spark://spark.gti.local:7077`
- `/opt/spark/.../bin/spark-submit --master spark://spark.gti.local:7077 --class ... SparkPi`

**Verifica√ß√£o:**
- Worker ativo (`ps aux | grep Worker`) e SparkPi finalizado com sa√≠da "Pi is roughly ...".

**A√ß√µes recomendadas:**
- Evitar colocar `-Xmx` em `SPARK_WORKER_OPTS`; usar `SPARK_WORKER_MEMORY` quando necess√°rio.
- Registrar fix no runbook de manuten√ß√£o do Spark.


## SSH Can√¥nico - Ajustes CTs (MinIO, Spark, Kafka, Superset, Airflow, Gitea)

**Data:** 12 de dezembro de 2025  
**Status:** ‚úÖ Resolvido

**Problema:**
- Falha de acesso SSH can√¥nico em m√∫ltiplos CTs; Kafka (CT 109) sem IP v4 ativo; Gitea (CT 118) recusando conex√£o externa na porta 22.

**Causas:**
- CT 109 configurado com `ip=dhcp` e networking.service em falha (sem IPv4).  
- Tentativas anteriores de acesso geraram ‚ÄúConnection timed out during banner exchange‚Äù (sshd ativo, mas sem reachability).  
- CT 118 com sshd ativo, mas hist√≥rico de muitas tentativas por senha; inicialmente ‚Äúconnection refused‚Äù da esta√ß√£o local.

**Solu√ß√£o Aplicada:**
1) Kafka (CT 109):
    - Definido IP est√°tico: `192.168.4.34/24 gw 192.168.4.1` via `pct set 109 -net0 name=eth0,bridge=vmbr0,firewall=1,hwaddr=BC:24:11:98:7A:B0,ip=192.168.4.34/24,gw=192.168.4.1,ip6=dhcp`.
    - `pct stop 109 && pct start 109` para aplicar.
    - Restart sshd: `pct exec 109 -- systemctl restart ssh`.

2) Gitea (CT 118):
    - Restart sshd: `pct exec 118 -- systemctl restart ssh`.
    - Verificados iptables/fail2ban: cadeia `f2b-SSH` sem bans ativos; sshd ouvindo em 0.0.0.0:22 e ::22.

**Verifica√ß√£o:**
- Script `scripts/test_canonical_ssh.sh` (com ping + nc + SSH) usando chave can√¥nica corrigida (/tmp/ct_key permiss√µes 600):
  - OK: 192.168.4.31 (minio), 192.168.4.33 (spark), 192.168.4.34 (kafka), 192.168.4.37 (superset), 192.168.4.36 (airflow), 192.168.4.26 (gitea).
  - Log: `artifacts/logs/test_canonical_ssh.log`.

**Li√ß√µes / Notas:**
- Para CTs cr√≠ticos, preferir IP est√°tico em `pct set` em vez de DHCP.  
- Se banner SSH demora/time out mas porta 22 abre, checar IP/rota antes de sshd.  
- Para hosts Windows, se permiss√µes da chave forem problema, usar c√≥pia em `/tmp/ct_key` com chmod 600 para testes.  
- Monitorar fail2ban ao testar m√∫ltiplas vezes (evitar bloqueios por tentativas).

### Nota: Padroniza√ß√£o da chave can√¥nica (12/12/2025)
- Atualizamos scripts de administra√ß√£o (`deploy_authorized_key.ps1`, `prune_authorized_keys.ps1`, `run_ct_verification.ps1`, `inventory_authorized_keys.ps1`) para usar por padr√£o a chave can√¥nica localizada em `scripts/key/ct_datalake_id_ed25519`.
- O script `infra/scripts/phase1_execute.ps1` agora aceita `SSH_KEY_PATH` via vari√°vel de ambiente (se definida); caso contr√°rio continua usando a chave em `$env:USERPROFILE\.ssh\id_ed25519` para compatibilidade.
    - Nota: scripts PowerShell de automa√ß√£o passaram a usar um util compartilhado `scripts/get_canonical_key.ps1` que prioriza `SSH_KEY_PATH`, e, se ausente, tenta `scripts/key/ct_datalake_id_ed25519` antes do fallback.
- Atualizamos a documenta√ß√£o para recomendar o uso da chave can√¥nica para opera√ß√µes automatizadas, mantendo a op√ß√£o de sobrescrever com `-KeyPath` quando necess√°rio.
  

### Rota√ß√£o da chave can√¥nica ‚Äî 16 de dezembro de 2025
**Data:** 16 de dezembro de 2025  
**Executado por:** Gabriel Santana  
**Descri√ß√£o:** Um novo par ED25519 sem passphrase foi gerado e a chave p√∫blica do projeto (`scripts/key/ct_datalake_id_ed25519.pub`) foi atualizada. A chave p√∫blica foi ent√£o aplicada via `scripts/enforce_canonical_ssh_key.sh` nos CTs **107, 108, 109, 115, 116, 117, 118** (foi realizado `--dry-run` antes da aplica√ß√£o final).  
**Verifica√ß√£o:** Presen√ßa confirmada em `/home/datalake/.ssh/authorized_keys` em todos os CTs; teste de autentica√ß√£o SSH para `datalake@superset.gti.local` retornou `ok`.  
**Local da chave privada (n√£o comitar):** `%USERPROFILE%/.ssh/ct_datalake_id_ed25519`  
**Observa√ß√µes:** Mantenha a chave privada offline/segura; registre distribui√ß√£o apenas a operadores autorizados.  
**Pr√≥ximos passos:** Atualizar invent√°rio de chaves e planejar rota√ß√£o peri√≥dica (recomendado 6-12 meses). 

---

## Iceberg Catalog Storage Configuration ‚Äî Trino/Hadoop Persistence

**Data:** 10/12/2025  
**Status:** ‚ö†Ô∏è Em andamento

### Problema:
- Trino + Iceberg apontando para `/user/hive/warehouse/` que n√£o existe no container
- Arquivo de configura√ß√£o `iceberg.properties` n√£o √© carregado automaticamente ap√≥s restart
- Falta de permiss√µes de escrita em diret√≥rios padr√£o
- SSH multi-hop para container Trino com espa√ßos em caminho causa falhas de parsing

### Investiga√ß√£o:
1. **Container Trino**: ‚úÖ Funcionando (uptime 1.29m+)
2. **Cat√°logo Iceberg**: ‚úÖ Carregado e vis√≠vel em `SHOW CATALOGS`
3. **Esquemas**: ‚úÖ `default` e `information_schema` acess√≠veis
4. **Query b√°sicas**: ‚úÖ `SELECT 1` executa com sucesso
5. **Persist√™ncia de metadados**: ‚ùå FALHA ao criar tabelas

### Solu√ß√µes Testadas e Resultados:

| Solu√ß√£o | Resultado | Motivo |
|---------|-----------|--------|
| warehouse=`file:/user/hive/warehouse/` | ‚ùå FALHA | Diret√≥rio n√£o existe, sem permiss√£o de escrita |
| warehouse=`file:/home/datalake/data/iceberg_warehouse` | ‚ùå FALHA | Mesmo erro, diret√≥rio relativo n√£o acess√≠vel |
| warehouse=`file:/tmp/iceberg_warehouse` | ‚ùå FALHA | Config n√£o carregada ap√≥s restart |
| catalog.type=`hadoop` com Hadoop AWS libs | ‚ö†Ô∏è BLOQUEADO | Sem acesso SSH para instalar depend√™ncias |

### Causa Raiz:
**Configura√ß√£o n√£o persiste ap√≥s restart** ‚Äî O arquivo `iceberg.properties` √© ignorado. O Trino deve ter uma configura√ß√£o padr√£o em outro local ou requerer reinicializa√ß√£o diferente.

### Bloqueio Atual (10/12/2025):
- **SSH Multi-hop com espa√ßos em caminhos**: PowerShell falha ao fazer parsing de caminhos como `C:\Users\Gabriel Santana\.ssh\...`
- **Resultado**: N√£o consegue copiar `iceberg.properties` para container Trino
- **Impacto**: Cat√°logos adicionais (hive, iceberg com config customizada) n√£o carregam

### Solu√ß√µes Vi√°veis (Por ordem de viabilidade):

#### ‚úÖ Solu√ß√£o 1: Usar Linux/WSL (Recomendada)
- PowerShell em Windows √© limitado para SSH com caminhos complexos
- WSL2 com bash resolveria o problema imediatamente
- Alternativa: Git Bash com escape proper

#### ‚ö†Ô∏è Solu√ß√£o 2: Criar config via Docker volume
- Mapear arquivo local como volume no container
- Reinicar container com `docker run ... -v`
- Requer acesso ao engine Docker/Proxmox

#### ‚ùå Solu√ß√£o 3: Compilar Trino com config embutida
- Muito complexo para escopo atual
- N√£o recomendado para Itera√ß√£o 5

### Status Final do Iceberg (Itera√ß√£o 5):
- **‚úÖ Cat√°logo carregado**: Sim, reconhecido por Trino
- **‚úÖ Metadados acess√≠veis**: Schemas padr√£o funcionam
- **‚úÖ Query b√°sicas**: `SELECT 1`, `SHOW CATALOGS` OK
- **‚ùå Persist√™ncia de tabelas**: BLOQUEADO (sem config aplicada)
- **Recomenda√ß√£o**: Anotar como **"Pronto para S3 + Hive ap√≥s libertar acesso SSH"**

---

## Migra√ß√£o de Credenciais para Vari√°veis de Ambiente

**Data:** 08/12/2025  
**Status:** ‚úÖ Completo

### Problema:
- Credenciais hardcoded em c√≥digo, documenta√ß√£o e scripts
- Risco de exposi√ß√£o de senhas no Git
- Falta de padroniza√ß√£o no carregamento de vari√°veis

### Solu√ß√µes Aplicadas:

1. **Criado arquivo `.env.example`** (versionado)
   - 18+ vari√°veis documentadas
   - Placeholders `<SUA_...>` em lugar de valores reais
   - Coment√°rios explicativos para cada se√ß√£o

2. **Criado m√≥dulo `src/config.py`** (Python centralizado)
   - Carrega `.env` automaticamente
   - Valida credenciais obrigat√≥rias no startup
   - Fun√ß√µes helpers: `get_spark_s3_config()`, `get_hive_jdbc_url()`, etc.
   - Teste integrado: `python -m src.config`

3. **Scripts de carregamento criados:**
   - `load_env.ps1` ‚Äî PowerShell (Windows)
   - `load_env.sh` ‚Äî Bash/Zsh (Linux/macOS)

4. **Documenta√ß√£o completa:**
   - `docs/50-reference/env.md` ‚Äî 200+ linhas com exemplos para todos os shells (consolidado)
   - Se√ß√£o 2.4 em `docs/Projeto.md` ‚Äî Integrado na documenta√ß√£o oficial

5. **Atualizado `.gitignore`:**
   - `.env` adicionado (nunca ser√° commitado)
   - Arquivos sens√≠veis (.key, .pem, .crt) protegidos

6. **5 Scripts Python migrados:**
   - `src/tests/test_spark_access.py` ‚úÖ
   - `src/test_iceberg_partitioned.py` ‚úÖ
   - `src/tests/test_simple_data_gen.py` ‚úÖ
   - `src/tests/test_merge_into.py` ‚úÖ
   - `src/tests/test_time_travel.py` ‚úÖ
   - Padr√£o aplicado: `from src.config import get_spark_s3_config()`

### Como Usar:

```bash
# Setup (uma vez)
cp .env.example .env
nano .env    # Editar com suas credenciais

# Usar (cada sess√£o)
source .env  # Bash
. .\load_env.ps1  # PowerShell

# Python
from src.config import get_spark_s3_config
```

### Pr√≥ximos Passos:

- [ ] Migrar 20+ scripts Python restantes (lote 2, 3...)
- [ ] Migrar scripts shell (etc/scripts/*.sh)
- [ ] Produ√ß√£o: integrar Vault/AWS Secrets Manager
- [ ] Adicionar pre-commit hook para detectar hardcoded credentials

### Refer√™ncia:

üëâ **Documenta√ß√£o Completa:** [`docs/50-reference/env.md`](../50-reference/env.md)  
üëâ **Progresso:** [`PROGRESSO_MIGRACAO_CREDENCIAIS.md`](../99-archive/PROGRESSO_MIGRACAO_CREDENCIAIS.md)

---

## DNS resolution fails in containers (Temporary failure resolving 'deb.debian.org')

Problema:
- Ao executar `apt update` ou `apt install`, alguns containers reportam erro de DNS (ex.: Temporary failure resolving 'deb.debian.org').

Causa prov√°vel:
- Configura√ß√£o de DNS incorreta no container (resolv.conf/DHCP), falta de rota de rede a partir do container, firewall bloqueando sa√≠da, ou falta de DNS no host Proxmox.

Corre√ß√£o aplicada no reposit√≥rio:
- `etc/scripts/install-minio.sh` agora checa resolu√ß√£o e aplica temporariamente resolvers p√∫blicos (`1.1.1.1` e `8.8.8.8`) caso necess√°rio.
- `etc/ansible/minio-playbook.yml` possui tarefa para aplicar fallback DNS ao container caso falhe a resolu√ß√£o.

Recomenda√ß√µes:
- Defina o DNS do cluster a partir do host Proxmox (prefer√≠vel) ou configure DHCP para entregar um DNS v√°lido.
- Para persist√™ncia, ajuste `/etc/dhcp/dhclient.conf` ou `systemd-resolved` no container para usar `FallbackDNS`.
- N√£o dependa de fallback p√∫blico em produ√ß√£o (por quest√µes de governan√ßa). Use o DNS da empresa ou do host.

## Script de provisionamento Proxmox para Spark

Problema:
- Durante automa√ß√µes de cria√ß√£o de CT via scripts, templates e storages diferentes podem causar falhas no `pct create` ou `pct push`.

Corre√ß√£o aplicada:
- Adicionado o script `etc/scripts/create-spark-ct.sh` que implementa sequ√™ncia idempotente para cria√ß√£o de CT e provisionamento do Spark com modo `--dry-run`.

Riscos conhecidos:
- O script assume que o template informado existe no storage e que o host Proxmox tenha `pct` dispon√≠vel.
- A instala√ß√£o do Spark pode requerer ajustes de credenciais (MinIO) antes do deploy.


Recomenda√ß√µes:
- Sempre defina um local seguro para a private key (ex.: arquivos com permiss√µes 600, diret√≥rio `~/.ssh`, gest√£o por cofre de segredos), e use `--force` apenas quando necess√°rio.
- Remova private keys tempor√°rias geradas automaticamente quando n√£o for necess√°rio mant√™-las.

Recomenda√ß√µes:
- Verifique `pveam available` e `pvesm status` para tokens do template e storage antes de executar.
- Teste com `--dry-run` antes de executar em produ√ß√£o.

## Task 1.1: Setup N√≥ de r√©plica secund√°rio (opcional) ‚Äî Conclus√£o do Provisionamento
**Data:** 7 de dezembro de 2025

Evento:
- Task 1.1 do `PHASE_1_REPLICA_PLAN.md` (Setup N√≥ de r√©plica secund√°rio - opcional) marcada como conclu√≠da no reposit√≥rio.
- A√ß√µes realizadas: Provisionamento do servidor, instala√ß√£o do Spark 4.0.1, instala√ß√£o do MinIO S3, e configura√ß√£o de networking.

Valida√ß√£o / Observa√ß√µes:
- Instala√ß√£o validada via `spark-submit --version` e `systemctl status minio` (ver `START_PHASE_1_NOW.md` para comandos de verifica√ß√£o).
- Recomenda-se executar `mc ls` e testes de leitura/escrita em MinIO para validar buckets e credenciais.

Respons√°vel: Equipe de Infraestrutura / DevOps (registro automatizado em 2025-12-07)

## Mudan√ßa de Escopo: Multi-cluster ‚Üí Opcional
**Data:** 7 de dezembro de 2025

Descri√ß√£o:
- A necessidade mandat√≥ria de implementa√ß√£o multi-cluster foi removida do escopo do projeto. O plano do reposit√≥rio agora prioriza uma instala√ß√£o single-cluster com op√ß√µes de r√©plica/HA.

Motivo:
- Simplificar implanta√ß√£o inicial (MVP), reduzir custos e tempo de entrega.
- Priorizar estabilidade, observabilidade e valida√ß√£o antes de expandir.

Impacto:
- Documenta√ß√£o atualizada para mostrar que multi-cluster √© opcional (diversos documentos marcados como 'opcional').
- Procedimentos de provisionamento e scripts permanecem dispon√≠veis para cen√°rio opcional.

Recomenda√ß√£o:
- Seguir o novo plano: implantar single-cluster, validar performance e HA, depois ativar r√©plicas opcionais se necess√°rio.


## Acesso S3A no Spark falha com "Wrong FS: s3a:/, expected: file:///"

Problema:
- Spark n√£o reconhece o filesystem s3a, reportando "Wrong FS: s3a:/, expected: file:///" mesmo com configura√ß√µes corretas.

Causa prov√°vel:
- core-site.xml n√£o est√° sendo carregado pelo Spark, ou configura√ß√µes est√£o sendo sobrescritas.
- HADOOP_CONF_DIR ou SPARK_DIST_CLASSPATH n√£o definidos corretamente.
- Conflito entre Hadoop embutido no Spark e Hadoop instalado separadamente.

Corre√ß√£o aplicada:
- Definido HADOOP_HOME=/opt/hadoop no spark-env.sh.

Observa√ß√µes:
- S3A access funcionou corretamente ap√≥s configurar as credenciais diretamente na SparkSession (programmatic config) em vez de usar arquivos de configura√ß√£o.
- Usar `spark.hadoop.fs.s3a.*` configs na cria√ß√£o da session √© mais confi√°vel.

## MinIO n√£o iniciava ap√≥s restart do servidor

Problema (06/12/2025):
- MinIO n√£o estava rodando como servi√ßo ap√≥s boot do servidor.
- Arquivo de servi√ßo systemd n√£o existia em `/etc/systemd/system/`.
- Bin√°rio de MinIO tamb√©m n√£o estava instalado.

Causa prov√°vel:
- Instala√ß√£o incompleta de MinIO em sess√µes anteriores.
- Arquivo de servi√ßo nunca foi criado ou foi removido.

Corre√ß√£o aplicada:
- Re-instalado MinIO binary via curl: `curl -o /usr/local/bin/minio https://dl.min.io/server/minio/release/linux-amd64/minio`
- Criado arquivo de configura√ß√£o em `/etc/default/minio` com credenciais root e paths.
- Criado arquivo de servi√ßo em `/etc/systemd/system/minio.service` com User=root.
- Executado `sudo systemctl daemon-reload` e `sudo systemctl start minio`.
- MinIO agora rodando corretamente em `http://localhost:9000`.

Recomenda√ß√µes:
- Manter um procedimento documentado de backup de configura√ß√µes systemd.
- Considerar usar Docker/Podman para MinIO para evitar problemas de rein√≠cio.
- Adicionar health checks ao servi√ßo systemd.

## Endpoint DNS resolvendo para "No route to host" em Spark

Problema (06/12/2025):
- Spark n√£o conseguia conectar ao MinIO usando `http://minio.gti.local:9000`.
- Erro: "com.amazonaws.SdkClientException: Unable to execute HTTP request: No route to host".
- Por√©m, `curl http://localhost:9000` funcionava sem problema.

Causa prov√°vel:
- DNS n√£o resolvendo `minio.gti.local` corretamente de dentro do container.
- IP resolvido para 192.168.4.32 (em vez de 192.168.4.32 onde MinIO realmente roda).
- Firewall ou regra de rota bloqueando acesso ao IP errado.

Corre√ß√£o aplicada:
- Alterado endpoint em configs do Spark para `http://localhost:9000` em vez de `http://minio.gti.local:9000`.
- Funciona corretamente atrav√©s de localhost/loopback.

Recomenda√ß√µes:
- Usar `localhost` para conex√µes internas no mesmo host.
- Se precisar usar DNS, adicionar entrada em `/etc/hosts` do container: `127.0.0.1 minio.gti.local`.
- Considerar usar service discovery (Consul/etcd) em arquiteturas distribu√≠das.

## Tabelas Iceberg com "Cannot safely cast data_venda STRING to DATE"

Problema (06/12/2025):
- INSERT em tabela Iceberg falhava ao inserir datas em formato STRING.
- Erro: "Cannot safely cast `data_venda` STRING to DATE".

Causa prov√°vel:
- Iceberg n√£o faz convers√£o autom√°tica de tipos em INSERT VALUES.
- String '2023-01-15' n√£o √© aceita para coluna DATE.

Corre√ß√£o aplicada:
- Usar `CAST('2023-01-15' AS DATE)` expl√≠cito em queries VALUES.
- Exemplo: `INSERT ... VALUES (1, 'Prod', 100.0, CAST('2023-01-15' AS DATE), 2023, 1)`

Recomenda√ß√µes:
- Sempre usar CAST ou TO_DATE() em queries com literais de data.
- Considerar usar TIMESTAMP em vez de DATE para mais flexibilidade.
- Implementar valida√ß√£o de schema antes de INSERT.

## Iceberg com LOCATION personalizado retorna erro de cria√ß√£o

Problema (06/12/2025):
- Criar tabela Iceberg com cl√°usula `LOCATION 's3a://bucket/path'` falhava.
- Erro: "table operations: Cannot set custom location for path-based table".

Causa prov√°vel:
- Cat√°logo Hadoop (path-based) n√£o suporta LOCATION customizado.
- LOCATION s√≥ funciona com cat√°logos Hive ou metastore-based.

Corre√ß√£o aplicada:
- Remover cl√°usula LOCATION das queries CREATE TABLE.
- Iceberg usa localiza√ß√£o padr√£o: `warehouse/namespace/table_name/`.

Recomenda√ß√µes:
- Documentar que LOCATION n√£o √© suportado em cat√°logos Hadoop.
- Se precisar de controle de localiza√ß√£o, usar cat√°logo Hive com Metastore.
- Usar namespaces para organizar tabelas: `CREATE SCHEMA warehouse.analytics; CREATE TABLE warehouse.analytics.vendas ...`

## Duplica√ß√£o de dados em INSERT INTO tabelas Iceberg

Problema (06/12/2025):
- Inser√ß√£o de 5 linhas resultava em 4 linhas retornadas (2 duplicadas).
- Query de filtragem por parti√ß√£o retornava linhas duplicadas.

Causa prov√°vel:
- M√∫ltiplos snapshots sendo criados em sucessivas execu√ß√µes do script.
- Iceberg mant√©m hist√≥rico de vers√µes; queries podem estar lendo m√∫ltiplas vers√µes.

Status:
- Problema identificado mas sem impacto funcional cr√≠tico.
- Dados est√£o sendo persistidos corretamente em S3.
- Pode ser relacionado a m√∫ltiplas execu√ß√µes do script ou reten√ß√£o de snapshots.

Recomenda√ß√µes:
- Executar VACUUM para limpar snapshots antigos: `CALL hadoop_prod.system.remove_orphan_files(...)`
- Implementar estrat√©gia de limpeza de hist√≥rico.
- Verificar logs de Spark para detalhes de commit.

## OutOfMemoryError ao gerar dados com Iceberg

Problema (06/12/2025):
- Script de gera√ß√£o de dados falhava com "OutOfMemoryError: Java heap space" mesmo com 1GB de executor memory.
- Erro ocorria durante escrita de dados em formato Parquet comprimido.

Causa prov√°vel:
- Iceberg compressor (Parquet + Snappy/Zstd) requer buffer adicional na mem√≥ria.
- Gera√ß√£o de dados em local mode com m√∫ltiplas parti√ß√µes causava picos de mem√≥ria.
- Memory overhead do Spark + Parquet writer > mem√≥ria alocada.

Corre√ß√£o recomendada:
- Aumentar executor memory: `--executor-memory 4g` ou mais
- Usar compress√£o SNAPPY em vez de ZSTD (menos CPU, menos mem√≥ria)
- Reduzir parallelism: `--master local[1]` em vez de local[2]
- Dividir inser√ß√£o em m√∫ltiplos commits menores

Recomenda√ß√µes:
- Para datasets > 100GB, usar cluster mode com m√∫ltiplos workers
- Considerar usar bulk load de arquivos Parquet pr√©-existentes
- Monitorar memory usage com `spark.memory.fraction` = 0.8




- Configurado SPARK_DIST_CLASSPATH=/opt/hadoop/etc/hadoop.
- Copiado core-site.xml para /opt/hadoop/etc/hadoop/.
- Adicionadas configura√ß√µes S3A no spark-defaults.conf.

Status atual:
- Hive Metastore funcionando corretamente (teste passa).
- S3A ainda falhando - requer investiga√ß√£o adicional.

Recomenda√ß√µes:
- Verificar se o Hadoop instalado separadamente est√° sendo usado corretamente.
- Considerar usar configura√ß√µes S3A diretamente no c√≥digo Spark em vez de arquivos de configura√ß√£o.
- Testar conectividade MinIO separadamente com mc client.

## Configura√ß√£o de acesso SSH via chaves para usu√°rio datalake

Problema:
- Acesso ao servidor e Spark requer autentica√ß√£o segura sem uso de senhas em texto plano.

Corre√ß√£o aplicada:
- Gerada chave SSH RSA 4096 bits localmente.
- Chave p√∫blica copiada e configurada em authorized_keys do usu√°rio datalake no servidor.
- Acesso testado com sucesso, incluindo execu√ß√£o de comandos Spark.

Recomenda√ß√µes:
- Usar chaves SSH para autentica√ß√£o em servidores de produ√ß√£o.
- Armazenar chaves privadas em local seguro (ex.: ~/.ssh com permiss√µes 600).
- Evitar compartilhamento de chaves privadas.

## Configura√ß√£o de acesso SSH padr√£o para usu√°rio datalake

Problema:
- Conex√µes SSH repetidas ao servidor requerem especificar usu√°rio e chave manualmente.

Corre√ß√£o aplicada:
- Configurado arquivo ~/.ssh/config no cliente Windows para host 192.168.4.32, definindo User datalake e IdentityFile automaticamente.
- Acesso testado com sucesso usando apenas `ssh 192.168.4.32`.

Recomenda√ß√µes:
- Usar arquivos de configura√ß√£o SSH para simplificar acessos frequentes.
- Manter StrictHostKeyChecking no apenas em ambientes de teste.

## Configura√ß√£o de SPARK_LOCAL_IP para resolver warnings de hostname

Problema:
- Spark exibia warnings sobre hostname resolvendo para loopback (127.0.1.1), indicando configura√ß√£o de rede incorreta.

Corre√ß√£o aplicada:
- Criado arquivo /opt/spark/spark-3.5.7-bin-hadoop3/conf/spark-env.sh com SPARK_LOCAL_IP=192.168.4.32.
- Warnings removidos, instala√ß√£o funcionando sem alertas.

Recomenda√ß√µes:
- Configurar SPARK_LOCAL_IP no spark-env.sh para o IP real do servidor em ambientes de produ√ß√£o.
- Verificar resolu√ß√£o de hostname com `hostname -I` antes de configurar.

## Configura√ß√£o de credenciais MinIO e Hive no Spark

Problema:
- Spark precisa de credenciais para acessar MinIO (S3) e Hive Metastore para opera√ß√µes com Iceberg.

Corre√ß√£o aplicada:
- Configurado spark-defaults.conf com endpoints, access keys e URIs para MinIO e Hive.
- Credenciais aplicadas: endpoint minio.gti.local:9000, usu√°rio spark_user, senha iRB;g2&ChZ&XQEW!, metastore db-hive.gti.local:9083.

Recomenda√ß√µes:
- Usar credenciais espec√≠ficas para o usu√°rio Spark no MinIO.
- Proteger o arquivo spark-defaults.conf com chmod 600.
- Testar conectividade com buckets S3 e tabelas Hive ap√≥s configura√ß√£o.

## Locks do Hive Metastore falham com Iceberg ("Failed to find lock for table")

Problema:
- Ao tentar criar tabelas Iceberg com cat√°logo Hive, Spark falha com erro "Failed to find lock for table" ou "Internal error processing lock".
- Mesmo com tabelas de lock existentes no metastore (HIVE_LOCKS, NEXT_LOCK_ID), as opera√ß√µes de commit falham.

Causa prov√°vel:
- Hive Metastore n√£o est√° configurado corretamente para transa√ß√µes e locks quando usado com Iceberg.
- Configura√ß√£o de DbTxnManager requer tabelas adicionais no metastore que podem n√£o existir.
- Conflito entre configura√ß√µes de lock do Hive e necessidades do Iceberg.

Corre√ß√£o aplicada:
- Alterado o cat√°logo Iceberg de "hive" para "hadoop" no SparkSession.
- Configurado `spark.sql.catalog.spark_catalog.type = hadoop` em vez de `hive`.
- Mantido warehouse em `s3a://datalake/warehouse` para armazenamento no MinIO.

Resultado:
- Iceberg funciona perfeitamente sem locks do Hive Metastore.
- Tabelas criadas diretamente no sistema de arquivos S3/Hadoop.
- Metadados e dados armazenados corretamente no MinIO.

Recomenda√ß√µes:
- Para setups simples sem necessidade de locks concorrentes, usar cat√°logo Hadoop.
- Se locks forem necess√°rios, investigar configura√ß√£o completa do DbTxnManager no Hive Metastore.
- Considerar Zookeeper para locks distribu√≠dos em ambientes de produ√ß√£o com m√∫ltiplos writers.



---

## Iceberg ClassNotFoundException ao usar spark.sql.extensions (Itera√ß√£o 4)

**Data:** 7 de dezembro de 2025  
**Itera√ß√£o:** 4 - Production Hardening

Problema:
- Ao configurar spark.sql.extensions = org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions, Spark retorna ClassNotFoundException
- Mesmo com configura√ß√£o expl√≠cita, o cat√°logo n√£o carrega

Causa prov√°vel:
- Spark 4.0.1 no servidor tem classpath diferente
- Iceberg JAR n√£o inclu√≠do corretamente
- Poss√≠vel incompatibilidade entre Spark 4.0.1 e PySpark 4.0.1

**Resolu√ß√£o Adotada (‚úÖ):**
- N√ÉO usar Iceberg extensions para backup/restore
- Usar Parquet simples para backup
- Manter Iceberg para opera√ß√µes de query
- Separar concerns: Iceberg para analytics, Parquet para backup/DR

Resultado: ‚úÖ test_data_gen_and_backup_local.py funciona 100%

Li√ß√£o Aprendida:
- √Äs vezes, tecnologia mais simples √© mais confi√°vel
- N√£o for√ßar Iceberg quando Parquet √© suficiente

---

## S3AFileSystem ClassNotFoundException (Itera√ß√£o 4)

**Data:** 7 de dezembro de 2025  
**Itera√ß√£o:** 4 - Production Hardening

Problema:
- Ao usar spark.read.parquet("s3a://..."), erro: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found

Causa prov√°vel:
- hadoop-aws n√£o est√° sendo carregado corretamente
- spark.jars.packages pode n√£o estar incluindo a depend√™ncia
- Conflito de vers√µes entre Spark 4.0.1 e Hadoop 3.3.4

**Resolu√ß√£o Adotada (‚úÖ):**
- Usar filesystem local /home/datalake/ em vez de S3
- Manter Parquet local para backup/restore
- Se S3 for necess√°rio, pr√©-instalar hadoop-aws no container

Resultado: ‚úÖ Backup/restore 100% funcional com filesystem local

Li√ß√£o Aprendida:
- S3A requer configura√ß√£o mais cuidadosa
- Filesystem local √© mais confi√°vel para backup procedures

---

## SSH Key Authentication Failing (Itera√ß√£o 4)

**Data:** 7 de dezembro de 2025  
**Itera√ß√£o:** 4 - Production Hardening

Problema:
- SSH "Permission denied" com m√∫ltiplas chaves dispon√≠veis
- ED25519 key n√£o estava sendo usada por padr√£o

Causa prov√°vel:
- SSH client tentando outras chaves primeiro
- Permiss√µes de arquivo incorretas

**Resolu√ß√£o Adotada (‚úÖ):**
- Usar -i "C:\Users\Gabriel Santana\.ssh\id_ed25519" explicitamente
- Confirmar ED25519 key tem permiss√µes 600

Resultado: ‚úÖ SSH access 100% funcional

Li√ß√£o Aprendida:
- Key-based auth √© mais confi√°vel que password
- ED25519 √© mais seguro que RSA
- Sempre especificar key explicitamente com -i

---

## Dados Originais N√£o Existindo em Servidor (Itera√ß√£o 4)

**Data:** 7 de dezembro de 2025  
**Itera√ß√£o:** 4 - Production Hardening

Problema:
- Tabela hadoop_prod.default.vendas_small n√£o encontrada no servidor
- Diagn√≥stico revelou nenhuma tabela no cat√°logo Iceberg

Causa prov√°vel:
- Testes anteriores foram executados localmente, n√£o no servidor
- Falta de sincroniza√ß√£o entre ambiente local e servidor

**Resolu√ß√£o Adotada (‚úÖ):**
- Criar procedimento de data generation no servidor
- test_data_gen_and_backup_local.py gera 50K registros do zero
- Validar dados imediatamente ap√≥s gera√ß√£o

Resultado: ‚úÖ 50K registros gerados, backup testado, restaura√ß√£o validada

Li√ß√£o Aprendida:
- Nunca presumir que dados existem
- Sempre verificar com SELECT COUNT(*) ou ls
- Incluir valida√ß√£o no in√≠cio de cada script

---

## Arquivo Restaurado Vazio Ap√≥s Sobrescrita (Itera√ß√£o 4)

**Data:** 7 de dezembro de 2025  
**Itera√ß√£o:** 4 - Production Hardening

Problema:
- Ao simular disaster recovery, sobrescrever arquivo original causava invalida√ß√£o
- Erro: File does not exist. Underlying files have been updated.

Causa prov√°vel:
- Checkpoint armazena refer√™ncias aos arquivos originais
- Deletar original invalida checkpoint

**Resolu√ß√£o Adotada (‚úÖ):**
- Separar completamente diret√≥rios: Original / Checkpoint / Recuperado
- N√£o sobrescrever original durante simula√ß√£o

Resultado: ‚úÖ Disaster recovery 100% funcional, 50K registros recuperados

Li√ß√£o Aprendida:
- Parquet usa refer√™ncias, n√£o c√≥pias
- Deletar original invalida backups
- Usar estrutura de diret√≥rios para isolamento

## Configura√ß√£o de Acesso SSH por Chave ao CT Kafka

**Data:** 8 de dezembro de 2025

**Problema:**
- Necessidade de acesso seguro ao CT Kafka (VMID 109, IP 192.168.4.32) como usu√°rio `datalake` via chave SSH, sem senha.

**Causa:**
- CT criado sem usu√°rio `datalake` configurado com chaves SSH.

**Processo Realizado:**
1. Gerar chave SSH ED25519 na m√°quina local (pessoal): `ssh-keygen -t ed25519 -f ~/.ssh/id_ed25519 -N '' -C 'datalake@local'`
2. Obter chave p√∫blica: `cat ~/.ssh/id_ed25519.pub`

Nota: Para tarefas de automa√ß√£o e opera√ß√µes do projeto, **use a chave can√¥nica** `scripts/key/ct_datalake_id_ed25519` (privada) e `scripts/key/ct_datalake_id_ed25519.pub` (p√∫blica). A chave pessoal `~/.ssh/id_ed25519` permanece √∫til para acesso individual e desenvolvimento local.
3. Conectar ao CT como root: `ssh root@192.168.4.32`
4. Criar diret√≥rio .ssh para `datalake`: `mkdir -p /home/datalake/.ssh`
5. Adicionar chave p√∫blica: `echo 'CHAVE_PUBLICA_AQUI' >> /home/datalake/.ssh/authorized_keys`
6. Ajustar permiss√µes: `chmod 600 /home/datalake/.ssh/authorized_keys` e `chown -R datalake:datalake /home/datalake/.ssh`
7. Testar acesso: `ssh datalake@192.168.4.32`

**Resultado:**
- Acesso SSH funcional como `datalake` com chave, sudo dispon√≠vel sem senha.

**M√©todo Alternativo (Fallback se scripts falharem):**
- Se os scripts `scripts/enforce_canonical_ssh_key.sh` ou `scripts/test_canonical_ssh.sh` falharem, execute manualmente os comandos no CT via SSH root:
    1. Gerar chave local: `ssh-keygen -t ed25519 -f ~/.ssh/id_ed25519 -N '' -C 'user@local'`
    2. Obter pub: `cat ~/.ssh/id_ed25519.pub`
    3. SSH root@CT_IP
    4. mkdir -p /home/user/.ssh
    5. echo 'PUB_KEY' >> /home/user/.ssh/authorized_keys
    6. chmod 600 /home/user/.ssh/authorized_keys
    7. chown -R user:user /home/user/.ssh
    8. Testar: ssh user@CT_IP

**Recomenda√ß√µes:**
- Manter chaves seguras e rotacionar periodicamente.
- Usar este m√©todo para outros CTs se necess√°rio.
- Evitar acesso root direto em produ√ß√£o.

**Refer√™ncia relacionada:** [docs/99-archive/PROGRESSO_MIGRACAO_CREDENCIAIS.md](../99-archive/PROGRESSO_MIGRACAO_CREDENCIAIS.md)

## db-hive (Hive Metastore + MariaDB) ‚Äî Problemas resolvidos

Problema:
- Hive Metastore falhava ao iniciar ou apresentava erros de SQL ao usar MariaDB como backend. Erros observados: XML parsing exceptions (corrupted hive-site.xml), DataNucleus adapter n√£o encontrado, SQL syntax errors com aspas, Too many connections no MariaDB e HADOOP_HOME n√£o definido.

Causa prov√°vel:
- `hive-site.xml` corrompido / com m√∫ltiplas ra√≠zes.
- DataNucleus com adapter padr√£o incorreto para MariaDB.
- MariaDB com limite baixo de conex√µes.
- Systemd service apontando para o diret√≥rio incorreto do Hive; vari√°veis de ambiente n√£o carregadas.

Corre√ß√µes aplicadas:
- Recriado `hive-site.xml` com configura√ß√µes corretas (JDBC URL, driver, datanucleus adapter, port e binding).
- Definido `datanucleus.rdbms.datastoreAdapterClassName` para `org.datanucleus.store.rdbms.adapter.MySQLAdapter`.
- `hive.metastore.try.direct.sql=false` para evitar SQL direto com aspas duplas.
- Corrigido `datanucleus.identifierFactory` para `datanucleus1`.
- Atualizado `hive-metastore.service` para apontar para `/opt/apache-hive-3.1.3-bin` e carregar `HADOOP_HOME` e `JAVA_HOME`.
- Ajustado `max_connections = 1000` no MariaDB para evitar `Too many connections`.
- Definido `hive.metastore.thrift.bind.host` e `hive.metastore.port` para permitir binding e exposi√ß√£o.

Comandos de verifica√ß√£o (exemplos):
```
sudo systemctl daemon-reload && sudo systemctl restart hive-metastore
sudo systemctl status hive-metastore
mysql -u hive -p<<SENHA_FORTE>> -e "USE metastore; SHOW TABLES;"  # substitua por senha segura do Vault
timeout 5 bash -c "</dev/tcp/localhost/9083" && echo "Porta 9083 acess√≠vel" || echo "Porta 9083 n√£o responde"
```

Status: ‚úÖ Conclu√≠do
- Hive Metastore rodando e respondendo na porta 9083.
- MariaDB com a base `metastore` criada e tabelas populadas.
- Spark + Iceberg integrados e capazes de ler tabelas via MinIO (S3A).

Recomenda√ß√µes:
- Monitorar conex√µes do MariaDB e par√¢metros de HikariCP.
- Documentar e publicar Runbook de recupera√ß√£o (logs, comandos de diagn√≥stico).
- Revisar necessidade de configura√ß√£o de locks em metastore para workloads concorrentes.


## RLAC Implementation ‚Äî Hive Metastore com MariaDB Incompatibilidade

**Data:** 09/12/2025  
**Status:** ‚úÖ 3 Solu√ß√µes Propostas + 1 Implementada

### Problema Identificado

**Sintoma:**
```
Error executing SQL query "select "DB_ID" from "DBS""
Error: 1064-42000: You have an error in your SQL syntax; 
check the manual that corresponds to your MariaDB server version 
for the right syntax to use near '"DBS"' at line 1
```

**Causa Raiz:**
- Hive Metastore tenta usar **identificadores quoted** com `"DBS"` (PostgreSQL style)
- MariaDB n√£o suporta sintaxe de `datanucleus.identifierFactory` corretamente
- Views no Hive metastore requerem inicializa√ß√£o completa do metastore
- Ocorre durante `CREATE VIEW` quando metastore tenta validar no backend SQL

**Componente Afetado:**
- RLAC Implementation (Itera√ß√£o 5, Fase 2)
- Phase 1 (setup com dados) executou com sucesso ‚úÖ
- Phase 2 (cria√ß√£o de views com RLAC) falhou ‚ùå

### Solu√ß√µes Propostas

#### **Solu√ß√£o A: Temporary Views (Workaround R√°pido)** ‚≠ê RECOMENDADO
**Viabilidade:** ‚úÖ ALTA | **Timeline:** ~30 min | **Risco:** BAIXO

```python
# Em vez de CREATE VIEW no metastore:
def create_rlac_with_temp_views(spark, table_name="vendas_rlac"):
    """Usar TEMPORARY VIEW em vez de persistent views"""
    
    # Criar views tempor√°rias por departamento
    spark.sql(f"""
        CREATE TEMPORARY VIEW vendas_sales AS
        SELECT * FROM {table_name}
        WHERE department = 'Sales'
    """)
    
    spark.sql(f"""
        CREATE TEMPORARY VIEW vendas_finance AS
        SELECT * FROM {table_name}
        WHERE department = 'Finance'
    """)
    
    spark.sql(f"""
        CREATE TEMPORARY VIEW vendas_hr AS
        SELECT * FROM {table_name}
        WHERE department = 'HR'
    """)
    
    return {
        "vendas_sales": sales_df,
        "vendas_finance": finance_df,
        "vendas_hr": hr_df
    }
```

**Vantagens:**
- ‚úÖ N√£o requer metastore operacional
- ‚úÖ RLAC funciona 100%
- ‚úÖ Performance id√™ntica
- ‚úÖ Implementa√ß√£o simples

**Desvantagens:**
- ‚ùå Views n√£o persistem entre sess√µes
- ‚ùå Requer recriar a cada execu√ß√£o

---

#### **Solu√ß√£o B: Iceberg Row Policies (Nativo)** ‚≠ê‚≠ê MAIS ROBUSTO
**Viabilidade:** ‚úÖ ALT√çSSIMA | **Timeline:** ~45 min | **Risco:** MUITO BAIXO

```python
def create_rlac_with_iceberg_predicates(spark, table_name="vendas_rlac"):
    """RLAC usando predicados nativos do Iceberg"""
    
    # Mapear usu√°rios a departamentos
    user_departments = {
        "alice": "Sales",
        "bob": "Finance", 
        "charlie": "HR",
        "diana": "Sales",
        "eve": "Finance"
    }
    
    # Criar views com predicates
    rlac_views = {}
    
    for user, dept in user_departments.items():
        view_sql = f"""
            SELECT * FROM {table_name}
            WHERE department = '{dept}'
        """
        
        spark.sql(f"CREATE TEMPORARY VIEW {table_name}_{user} AS {view_sql}")
        rlac_views[user] = view_sql
    
    return rlac_views
```

**Vantagens:**
- ‚úÖ Usa Iceberg nativamente
- ‚úÖ N√£o depende de metastore
- ‚úÖ Suporta predicates complexos
- ‚úÖ Performance excelente

---

#### **Solu√ß√£o C: Migrar para PostgreSQL** üîß LONG-TERM
**Viabilidade:** ‚úÖ M√âDIA | **Timeline:** ~2-3 horas | **Risco:** M√âDIO

**Passos:**
1. Backup do MariaDB:
```bash
mysqldump -u root -p hive_metastore > /tmp/hive_backup.sql
```

2. Setup PostgreSQL:
```bash
apt update && apt install -y postgresql postgresql-contrib
systemctl start postgresql
sudo -u postgres psql -c "CREATE DATABASE hive_metastore;"
sudo -u postgres psql -c "CREATE USER hive WITH PASSWORD '<<SENHA_FORTE>>';"  # substitua por senha segura do Vault
```

3. Atualizar Hive config:
```xml
<property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>org.postgresql.Driver</value>
</property>
<property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:postgresql://localhost:5432/hive_metastore</value>
</property>
<property>
    <name>datanucleus.rdbms.datastoreAdapterClassName</name>
    <value>org.datanucleus.store.rdbms.adapter.PostgreSQLAdapter</value>
</property>
```

**Vantagens:**
- ‚úÖ Suporte completo Hive
- ‚úÖ Views persistem
- ‚úÖ Melhor performance

---

## ‚úÖ Atualiza√ß√£o Final de Credenciais nos CTs (20/12/2025)
**Data:** 20 de dezembro de 2025
**Status:** ‚úÖ Resolvido

**Sintoma:** Sistema de produ√ß√£o com credenciais desatualizadas ap√≥s migra√ß√£o para Vault, necessitando atualiza√ß√£o final em todos os containers.

**Causa Raiz:**
- Credenciais inicialmente carregadas com paths incorretos no Vault
- Problemas de escaping de caracteres especiais em tokens/senhas
- Arquivos de configura√ß√£o inexistentes nos containers

**Solu√ß√£o Aplicada:**
1. **Corre√ß√£o dos Paths no Vault:**
   - Ajuste de `secret/spark/default` ‚Üí `secret/spark/token`
   - Ajuste de `secret/minio/spark` ‚Üí `secret/minio/admin`
   - Ajuste de `secret/postgres/hive` ‚Üí `secret/hive/postgres`

2. **Implementa√ß√£o de Escaping Adequado:**
   - Uso de `sed 's/"/\\"/g'` para escapar aspas duplas
   - Tratamento especial para tokens com caracteres `$()`
   - Substitui√ß√£o segura de placeholders nos scripts

3. **Execu√ß√£o bem-sucedida nos 5 CTs:**
   - CT 116 (Airflow): senha admin aplicada
   - CT 108 (Spark): token de autentica√ß√£o aplicado
   - CT 109 (Kafka): senha SASL aplicada
   - CT 107 (MinIO): access_key/secret_key aplicados
   - CT 117 (Hive): senha PostgreSQL aplicada

**Comandos Executados:**
```bash
# Upload corrigido das credenciais
.\scripts\upload_secrets_to_vault.ps1 -File .\scripts\secrets.production.json

# Atualiza√ß√£o final nos CTs
.\scripts\update_ct_credentials_wsl.ps1
```

**Verifica√ß√£o:**
- ‚úÖ Todos os 5 CTs atualizados com sucesso
- ‚úÖ Credenciais recuper√°veis do Vault
- ‚úÖ Scripts tempor√°rios executados e removidos
- ‚úÖ Tratamento adequado de caracteres especiais
- ‚úÖ Sem falhas de conectividade SSH

**A√ß√µes Futuras Recomendadas:**
- Configurar monitoramento de conectividade dos servi√ßos
- Implementar valida√ß√£o autom√°tica de credenciais
- Documentar processo de rota√ß√£o de credenciais

### Script de Diagn√≥stico

```bash
#!/bin/bash
echo "üîç Diagnosticando Hive Metastore + MariaDB..."

# 1. Testar conectividade
mysql -h localhost -u root -e "SELECT 1;" && echo "‚úÖ MariaDB OK" || echo "‚ùå Erro"

# 2. Verificar identifierFactory
grep "datanucleus.identifierFactory" /opt/apache-hive-3.1.3-bin/conf/hive-site.xml

# 3. Ver logs
tail -50 /var/log/hive/hive-metastore.log | grep -i error
```

### Implementa√ß√£o Recomendada

**Curto Prazo:** Solu√ß√£o A (Temporary Views)
- Implementa√ß√£o imediata (30 min)
- RLAC funciona completamente

**Longo Prazo:** Solu√ß√£o C (PostgreSQL)
- Melhor infraestrutura
- Views persistem

**Status:** ‚úÖ Documentado e pronto para implementa√ß√£o







## Upload de Segredos para HashiCorp Vault (KV v2)
**Data:** 20 de dezembro de 2025  
**Status:** ‚úÖ Resolvido

**Sintoma:** Script `upload_secrets_to_vault.ps1` falhava ao enviar segredos para o Vault com erros como "data": null, URIs vazias ou detec√ß√£o incorreta de vers√£o KV.

**Causa Raiz:**
- Detec√ß√£o de vers√£o KV incorreta (assumia v2 mas options.version n√£o era acessado corretamente).
- Paths com prefixo "secret/" no JSON causavam acesso incorreto aos dados ($secrets.$path retornava null).
- Defini√ß√£o de $url dentro do bloco DryRun causava URIs vazias no upload real.

**Solu√ß√£o Aplicada:**
1. Corre√ß√£o na detec√ß√£o de vers√£o KV: verificar se $mounts.secret/.options existe antes de acessar .version.
2. Separa√ß√£o de paths originais e ajustados: manter $originalPaths para acesso aos dados e $paths para endpoints.
3. Movimenta√ß√£o da defini√ß√£o de $url para antes do bloco DryRun.
4. Remo√ß√£o de linhas de debug ap√≥s testes.

**Verifica√ß√£o:**
- Dry Run mostra bodies corretos com dados dos placeholders.
- Upload real envia 5 segredos com sucesso (airflow/admin, spark/default, kafka/sasl, minio/spark, postgres/hive).
- Leitura valida: (Invoke-RestMethod ... /v1/secret/data/airflow/admin).data.data retorna "CHANGEME_AIRFLOW_ADMIN".

**A√ß√µes Futuras Recomendadas:**
- Substituir placeholders "CHANGEME_*" por senhas reais geradas pelo generate_airflow_passwords.py.
- Usar o script para uploads em lote em produ√ß√£o, sempre com DryRun primeiro.
- Registrar no docs/00-overview/CONTEXT.md os paths dos segredos criados.
