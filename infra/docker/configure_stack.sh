#!/bin/bash
# =============================================================================
# DATALAKE FB - Script de Configura√ß√£o Completa
# =============================================================================
# Este script configura automaticamente todos os componentes da stack ap√≥s
# o primeiro `docker compose up -d`.
#
# Uso:
#   ./configure_stack.sh
#
# =============================================================================

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"

echo "=========================================="
echo "üîß DATALAKE FB - Configura√ß√£o Autom√°tica"
echo "=========================================="
echo ""

# -----------------------------------------------------------------------------
# 1. GITEA - Ativar Install Lock e Criar Usu√°rio Admin
# -----------------------------------------------------------------------------
configure_gitea() {
    echo "1Ô∏è‚É£  Configurando Gitea..."
    
    # Aguardar Gitea estar pronto
    echo "   ‚è≥ Aguardando Gitea iniciar..."
    until curl -s http://localhost:3000 > /dev/null 2>&1; do
        sleep 3
    done
    
    # Ativar INSTALL_LOCK se necess√°rio
    INSTALL_LOCK=$(docker exec gitea grep "INSTALL_LOCK" /data/gitea/conf/app.ini 2>/dev/null | grep -c "true" || echo "0")
    if [ "$INSTALL_LOCK" == "0" ]; then
        echo "   üîí Ativando Install Lock..."
        docker exec gitea sed -i 's/INSTALL_LOCK = false/INSTALL_LOCK = true/' /data/gitea/conf/app.ini
        docker restart gitea
        sleep 5
    fi
    
    # Criar usu√°rio admin
    echo "   üë§ Criando usu√°rio admin..."
    docker exec -u git gitea gitea admin user create \
        --config /data/gitea/conf/app.ini \
        --username datalake_admin \
        --password DatalakeAdmin@2026 \
        --email admin@datalake.local \
        --admin 2>/dev/null || echo "   ‚ö†Ô∏è  Usu√°rio j√° existe"
    
    echo "   ‚úÖ Gitea configurado!"
    echo "      URL: http://localhost:3000"
    echo "      User: datalake_admin"
    echo "      Pass: DatalakeAdmin@2026"
}

# -----------------------------------------------------------------------------
# 2. SUPERSET - Copiar Scripts de RBAC
# -----------------------------------------------------------------------------
configure_superset() {
    echo ""
    echo "2Ô∏è‚É£  Configurando Superset..."
    
    # Aguardar Superset estar pronto
    echo "   ‚è≥ Aguardando Superset iniciar..."
    until docker exec datalake-superset ls /app 2>/dev/null; do
        sleep 5
    done
    
    # Copiar scripts de RBAC
    echo "   üìÑ Copiando scripts de configura√ß√£o..."
    docker cp "$PROJECT_ROOT/src/setup_superset_roles.py" datalake-superset:/app/ 2>/dev/null || true
    docker cp "$PROJECT_ROOT/src/setup_superset_assets.py" datalake-superset:/app/ 2>/dev/null || true
    
    # Verificar
    if docker exec datalake-superset ls /app/setup_superset_roles.py /app/setup_superset_assets.py > /dev/null 2>&1; then
        echo "   ‚úÖ Scripts copiados!"
    else
        echo "   ‚ö†Ô∏è  Scripts n√£o encontrados"
    fi
    
    echo "   ‚úÖ Superset configurado!"
    echo "      URL: http://localhost:8088"
    echo "      User: admin"
    echo "      Pass: admin"
}

# -----------------------------------------------------------------------------
# 3. MINIO - Criar Buckets
# -----------------------------------------------------------------------------
configure_minio() {
    echo ""
    echo "3Ô∏è‚É£  Configurando MinIO..."
    
    # Aguardar MinIO estar pronto
    echo "   ‚è≥ Aguardando MinIO iniciar..."
    until curl -s http://localhost:9000/minio/health/live > /dev/null 2>&1; do
        sleep 3
    done
    
    # O container mc j√° cria os buckets, mas vamos verificar
    echo "   üì¶ Verificando buckets..."
    docker exec datalake-mc mc ls local 2>/dev/null && echo "   ‚úÖ Buckets dispon√≠veis" || echo "   ‚ö†Ô∏è  Verificar buckets manualmente"
    
    echo "   ‚úÖ MinIO configurado!"
    echo "      URL: http://localhost:9001"
    echo "      User: datalake"
}

# -----------------------------------------------------------------------------
# 4. KAFKA CONNECT - Verificar Status
# -----------------------------------------------------------------------------
configure_kafka_connect() {
    echo ""
    echo "4Ô∏è‚É£  Verificando Kafka Connect..."
    
    # Aguardar Kafka Connect
    echo "   ‚è≥ Aguardando Kafka Connect iniciar..."
    until curl -s http://localhost:8083/connectors > /dev/null 2>&1; do
        sleep 5
    done
    
    echo "   ‚úÖ Kafka Connect online!"
    echo "      URL: http://localhost:8083"
}

# -----------------------------------------------------------------------------
# 5. SUPERSET - Configurar Conex√£o Trino
# -----------------------------------------------------------------------------
configure_superset_database() {
    echo ""
    echo "5Ô∏è‚É£  Configurando conex√£o Trino no Superset..."
    
    # Aguardar Superset estar healthy
    echo "   ‚è≥ Aguardando Superset ficar healthy..."
    until docker exec datalake-superset curl -s http://localhost:8088/health > /dev/null 2>&1; do
        sleep 5
    done
    
    # Executar script Python para criar a conex√£o via API
    docker exec datalake-superset /app/.venv/bin/python -c "
import requests
import json

# Login e obter CSRF token
session = requests.Session()
login_url = 'http://localhost:8088/api/v1/security/login'
login_data = {'username': 'admin', 'password': 'admin', 'provider': 'db', 'refresh': True}

try:
    resp = session.post(login_url, json=login_data)
    if resp.status_code == 200:
        token = resp.json().get('access_token')
        headers = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}
        
        # Obter CSRF Token explicitamente
        csrf_url = 'http://localhost:8088/api/v1/security/csrf_token/'
        csrf_resp = session.get(csrf_url, headers=headers)
        if csrf_resp.status_code == 200:
            csrf_token = csrf_resp.json().get('result')
            headers['X-CSRFToken'] = csrf_token
        
        # Verificar se j√° existe conex√£o Trino
        dbs_resp = session.get('http://localhost:8088/api/v1/database/', headers=headers)
        existing_dbs = dbs_resp.json().get('result', [])
        trino_exists = any(db.get('database_name') == 'Trino' for db in existing_dbs)
        
        if not trino_exists:
            # Criar conex√£o Trino
            db_data = {
                'database_name': 'Trino',
                'sqlalchemy_uri': 'trino://trino@datalake-trino:8080/iceberg',
                'expose_in_sqllab': True,
                'allow_ctas': True,
                'allow_cvas': True,
                'allow_dml': True
            }
            create_resp = session.post('http://localhost:8088/api/v1/database/', headers=headers, json=db_data)
            if create_resp.status_code in [200, 201]:
                print('Conexao Trino criada com sucesso!')
            else:
                print(f'Erro ao criar conexao: {create_resp.text}')
        else:
            print('Conexao Trino ja existe.')
    else:
        print(f'Erro no login: {resp.status_code}')
except Exception as e:
    print(f'Erro: {e}')
" 2>/dev/null || echo "   ‚ö†Ô∏è  Configura√ß√£o manual necess√°ria"
    
    echo "   ‚úÖ Conex√£o Trino configurada!"
}

# -----------------------------------------------------------------------------
# 6. TRINO/ICEBERG - Criar Schema e Tabelas
# -----------------------------------------------------------------------------
configure_iceberg_tables() {
    echo ""
    echo "6Ô∏è‚É£  Configurando tabelas Iceberg..."
    
    # Aguardar Trino estar pronto
    echo "   ‚è≥ Aguardando Trino iniciar..."
    until docker exec datalake-trino trino --execute "SELECT 1" > /dev/null 2>&1; do
        sleep 5
    done
    
    # Criar schema 'isp' para dados do ISP
    echo "   üì¶ Criando schema 'isp'..."
    docker exec datalake-trino trino --execute "CREATE SCHEMA IF NOT EXISTS iceberg.isp" 2>/dev/null || true
    
    # Criar tabelas para os dados do Datagen
    echo "   üìä Criando tabelas Iceberg..."
    
    # Tabela: customers (clientes)
    docker exec datalake-trino trino --execute "
    CREATE TABLE IF NOT EXISTS iceberg.isp.customers (
        id VARCHAR,
        name VARCHAR,
        email VARCHAR,
        phone VARCHAR,
        address VARCHAR,
        city VARCHAR,
        state VARCHAR,
        plan_type VARCHAR,
        status VARCHAR,
        created_at TIMESTAMP,
        updated_at TIMESTAMP
    ) WITH (
        format = 'PARQUET',
        partitioning = ARRAY['month(created_at)']
    )
    " 2>/dev/null || echo "   ‚ö†Ô∏è  Tabela customers j√° existe"
    
    # Tabela: sessions (sess√µes de conex√£o)
    docker exec datalake-trino trino --execute "
    CREATE TABLE IF NOT EXISTS iceberg.isp.sessions (
        id VARCHAR,
        customer_id VARCHAR,
        ip_address VARCHAR,
        mac_address VARCHAR,
        bytes_in BIGINT,
        bytes_out BIGINT,
        start_time TIMESTAMP,
        end_time TIMESTAMP,
        duration_seconds INTEGER,
        connection_type VARCHAR
    ) WITH (
        format = 'PARQUET',
        partitioning = ARRAY['day(start_time)']
    )
    " 2>/dev/null || echo "   ‚ö†Ô∏è  Tabela sessions j√° existe"
    
    # Tabela: invoices (faturas)
    docker exec datalake-trino trino --execute "
    CREATE TABLE IF NOT EXISTS iceberg.isp.invoices (
        id VARCHAR,
        customer_id VARCHAR,
        amount DECIMAL(10,2),
        due_date DATE,
        paid_date DATE,
        status VARCHAR,
        payment_method VARCHAR,
        created_at TIMESTAMP
    ) WITH (
        format = 'PARQUET',
        partitioning = ARRAY['month(created_at)']
    )
    " 2>/dev/null || echo "   ‚ö†Ô∏è  Tabela invoices j√° existe"
    
    # Tabela: contracts (contratos)
    docker exec datalake-trino trino --execute "
    CREATE TABLE IF NOT EXISTS iceberg.isp.contracts (
        id VARCHAR,
        customer_id VARCHAR,
        plan_name VARCHAR,
        speed_mbps INTEGER,
        monthly_price DECIMAL(10,2),
        start_date DATE,
        end_date DATE,
        status VARCHAR,
        created_at TIMESTAMP
    ) WITH (
        format = 'PARQUET',
        partitioning = ARRAY['year(start_date)']
    )
    " 2>/dev/null || echo "   ‚ö†Ô∏è  Tabela contracts j√° existe"
    
    # Listar tabelas criadas
    echo "   üìã Tabelas dispon√≠veis:"
    docker exec datalake-trino trino --execute "SHOW TABLES FROM iceberg.isp" 2>/dev/null | grep -v "^$" | sed 's/^/      ‚Ä¢ /'
    
    echo "   ‚úÖ Tabelas Iceberg configuradas!"
}

# -----------------------------------------------------------------------------
# 7. PIPELINE DE INGEST√ÉO - Spark Job para carregar dados
# -----------------------------------------------------------------------------
configure_data_pipeline() {
    echo ""
    echo "7Ô∏è‚É£  Configurando pipeline de ingest√£o..."
    
    # Criar script de ingest√£o Spark
    echo "   üìù Criando script de ingest√£o Spark..."
    
    cat > /tmp/ingest_data.py << 'SPARK_SCRIPT'
#!/usr/bin/env python3
"""
Spark Job: Ingest√£o de dados do Datagen para Iceberg
Este script gera dados de exemplo e insere nas tabelas Iceberg.
"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import random
import uuid
from datetime import datetime, timedelta
from decimal import Decimal

# Criar SparkSession com suporte a Iceberg
spark = SparkSession.builder \
    .appName("DataLake_ISP_Ingestion") \
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
    .config("spark.sql.catalog.iceberg", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.iceberg.type", "hive") \
    .config("spark.sql.catalog.iceberg.uri", "thrift://datalake-hive:9083") \
    .config("spark.sql.catalog.iceberg.warehouse", "s3a://warehouse/") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://datalake-minio:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "datalake") \
    .config("spark.hadoop.fs.s3a.secret.key", "iRB;g2&ChZ&XQEW!") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")
print("=" * 50)
print("üöÄ Iniciando ingest√£o de dados ISP...")
print("=" * 50)

# Gerar dados de exemplo
def generate_customers(n=100):
    cities = ["S√£o Paulo", "Rio de Janeiro", "Belo Horizonte", "Curitiba", "Porto Alegre", "Salvador", "Fortaleza"]
    states = ["SP", "RJ", "MG", "PR", "RS", "BA", "CE"]
    plans = ["B√°sico", "Padr√£o", "Premium", "Empresarial"]
    statuses = ["Ativo", "Inativo", "Suspenso"]
    
    data = []
    for i in range(n):
        city_idx = random.randint(0, len(cities)-1)
        data.append((
            str(uuid.uuid4()),
            f"Cliente {i+1}",
            f"cliente{i+1}@email.com",
            f"({random.randint(11,99)}) 9{random.randint(1000,9999)}-{random.randint(1000,9999)}",
            f"Rua {random.randint(1,999)}, {random.randint(1,500)}",
            cities[city_idx],
            states[city_idx],
            random.choice(plans),
            random.choice(statuses),
            datetime.now() - timedelta(days=random.randint(1, 365)),
            datetime.now()
        ))
    return data

def generate_sessions(n=500):
    data = []
    for i in range(n):
        start = datetime.now() - timedelta(hours=random.randint(1, 720))
        duration = random.randint(60, 86400)
        data.append((
            str(uuid.uuid4()),
            str(uuid.uuid4()),
            f"192.168.{random.randint(1,254)}.{random.randint(1,254)}",
            f"AA:BB:CC:{random.randint(10,99)}:{random.randint(10,99)}:{random.randint(10,99)}",
            random.randint(1000000, 50000000000),
            random.randint(100000, 5000000000),
            start,
            start + timedelta(seconds=duration),
            duration,
            random.choice(["Fibra", "R√°dio", "Cabo"])
        ))
    return data

def generate_invoices(n=200):
    data = []
    for i in range(n):
        created = datetime.now() - timedelta(days=random.randint(1, 180))
        due = created + timedelta(days=30)
        paid = due - timedelta(days=random.randint(-5, 10)) if random.random() > 0.2 else None
        amount = Decimal(str(round(random.uniform(79.90, 499.90), 2)))
        data.append((
            str(uuid.uuid4()),
            str(uuid.uuid4()),
            amount,
            due.date(),
            paid.date() if paid else None,
            "Pago" if paid else random.choice(["Pendente", "Atrasado"]),
            random.choice(["Boleto", "Cart√£o", "PIX", "D√©bito"]) if paid else None,
            created
        ))
    return data

def generate_contracts(n=100):
    plans = [
        ("Internet 100Mbps", 100, "89.90"),
        ("Internet 200Mbps", 200, "119.90"),
        ("Internet 500Mbps", 500, "179.90"),
        ("Internet 1Gbps", 1000, "299.90"),
        ("Empresarial 500Mbps", 500, "399.90")
    ]
    data = []
    for i in range(n):
        plan = random.choice(plans)
        start = datetime.now().date() - timedelta(days=random.randint(30, 730))
        data.append((
            str(uuid.uuid4()),
            str(uuid.uuid4()),
            plan[0],
            plan[1],
            Decimal(plan[2]),
            start,
            start + timedelta(days=365),
            random.choice(["Ativo", "Encerrado", "Cancelado"]),
            datetime.now() - timedelta(days=random.randint(30, 730))
        ))
    return data

# Schemas
customers_schema = StructType([
    StructField("id", StringType()), StructField("name", StringType()),
    StructField("email", StringType()), StructField("phone", StringType()),
    StructField("address", StringType()), StructField("city", StringType()),
    StructField("state", StringType()), StructField("plan_type", StringType()),
    StructField("status", StringType()), StructField("created_at", TimestampType()),
    StructField("updated_at", TimestampType())
])

sessions_schema = StructType([
    StructField("id", StringType()), StructField("customer_id", StringType()),
    StructField("ip_address", StringType()), StructField("mac_address", StringType()),
    StructField("bytes_in", LongType()), StructField("bytes_out", LongType()),
    StructField("start_time", TimestampType()), StructField("end_time", TimestampType()),
    StructField("duration_seconds", IntegerType()), StructField("connection_type", StringType())
])

invoices_schema = StructType([
    StructField("id", StringType()), StructField("customer_id", StringType()),
    StructField("amount", DecimalType(10,2)), StructField("due_date", DateType()),
    StructField("paid_date", DateType()), StructField("status", StringType()),
    StructField("payment_method", StringType()), StructField("created_at", TimestampType())
])

contracts_schema = StructType([
    StructField("id", StringType()), StructField("customer_id", StringType()),
    StructField("plan_name", StringType()), StructField("speed_mbps", IntegerType()),
    StructField("monthly_price", DecimalType(10,2)), StructField("start_date", DateType()),
    StructField("end_date", DateType()), StructField("status", StringType()),
    StructField("created_at", TimestampType())
])

# Inserir dados
print("\nüìä Inserindo clientes...")
customers_df = spark.createDataFrame(generate_customers(100), customers_schema)
customers_df.writeTo("iceberg.isp.customers").append()
print(f"   ‚úÖ {customers_df.count()} clientes inseridos")

print("\nüìä Inserindo sess√µes...")
sessions_df = spark.createDataFrame(generate_sessions(500), sessions_schema)
sessions_df.writeTo("iceberg.isp.sessions").append()
print(f"   ‚úÖ {sessions_df.count()} sess√µes inseridas")

print("\nüìä Inserindo faturas...")
invoices_df = spark.createDataFrame(generate_invoices(200), invoices_schema)
invoices_df.writeTo("iceberg.isp.invoices").append()
print(f"   ‚úÖ {invoices_df.count()} faturas inseridas")

print("\nüìä Inserindo contratos...")
contracts_df = spark.createDataFrame(generate_contracts(100), contracts_schema)
contracts_df.writeTo("iceberg.isp.contracts").append()
print(f"   ‚úÖ {contracts_df.count()} contratos inseridos")

print("\n" + "=" * 50)
print("‚úÖ Ingest√£o conclu√≠da com sucesso!")
print("=" * 50)

# Mostrar contagens finais
print("\nüìã Resumo das tabelas:")
for table in ["customers", "sessions", "invoices", "contracts"]:
    count = spark.sql(f"SELECT COUNT(*) FROM iceberg.isp.{table}").collect()[0][0]
    print(f"   ‚Ä¢ {table}: {count} registros")

spark.stop()
SPARK_SCRIPT

    # Copiar script para o container Spark
    echo "   üìÇ Criando diret√≥rio de trabalho..."
    docker exec datalake-spark-master mkdir -p /opt/spark/work-dir
    docker cp /tmp/ingest_data.py datalake-spark-master:/opt/spark/work-dir/
    
    # Executar o job Spark
    echo "   üöÄ Executando job de ingest√£o Spark..."
    docker exec datalake-spark-master /opt/spark/bin/spark-submit \
        --master local[*] \
        --conf spark.driver.memory=1g \
        --conf spark.executor.memory=1g \
        /opt/spark/work-dir/ingest_data.py 2>&1 | grep -E "(Inserindo|inseridos|Resumo|‚úÖ|‚Ä¢|üöÄ|üìä)" || echo "   ‚ö†Ô∏è  Job executado (verificar logs)"
    
    echo "   ‚úÖ Pipeline de ingest√£o configurado!"
}

# -----------------------------------------------------------------------------
# 8. SINCRONIZAR C√ìDIGO COM GITEA
# -----------------------------------------------------------------------------
sync_code_to_gitea() {
    echo ""
    echo "8Ô∏è‚É£  Sincronizando c√≥digo com Gitea..."
    
    # Aguardar Gitea estar pronto ap√≥s poss√≠vel restart
    sleep 5
    until curl -s http://localhost:3000 > /dev/null 2>&1; do
        sleep 3
    done
    
    # Criar reposit√≥rio via API
    echo "   üì¶ Criando reposit√≥rio..."
    curl -s -X POST "http://localhost:3000/api/v1/user/repos" \
        -H "content-type: application/json" \
        -u "datalake_admin:DatalakeAdmin@2026" \
        -d '{"name":"datalake-fb", "private": false}' > /dev/null 2>&1 || true
    
    # Configurar git local
    cd "$PROJECT_ROOT"
    git config user.email "bot@datalake.local" 2>/dev/null || true
    git config user.name "DataLake Bot" 2>/dev/null || true
    
    # Adicionar remote se n√£o existir
    git remote remove gitea_origin 2>/dev/null || true
    git remote add gitea_origin "http://datalake_admin:DatalakeAdmin%402026@localhost:3000/datalake_admin/datalake-fb.git"
    
    # Push
    git add . 2>/dev/null || true
    git commit -m "Auto-sync: Configura√ß√£o inicial" 2>/dev/null || true
    git push -u gitea_origin main --force 2>/dev/null && echo "   ‚úÖ C√≥digo sincronizado!" || echo "   ‚ö†Ô∏è  J√° sincronizado"
    
    echo "      Repo: http://localhost:3000/datalake_admin/datalake-fb"
}

# -----------------------------------------------------------------------------
# EXECUTAR CONFIGURA√á√ïES
# -----------------------------------------------------------------------------
main() {
    configure_gitea
    configure_superset
    configure_minio
    configure_kafka_connect
    configure_superset_database
    configure_iceberg_tables
    configure_data_pipeline
    sync_code_to_gitea
    
    echo ""
    echo "=========================================="
    echo "‚úÖ CONFIGURA√á√ÉO CONCLU√çDA!"
    echo "=========================================="
    echo ""
    echo "üìä URLs de Acesso:"
    echo "   ‚Ä¢ Superset:      http://localhost:8088"
    echo "   ‚Ä¢ Trino:         http://localhost:8081"
    echo "   ‚Ä¢ Kafka UI:      http://localhost:8090"
    echo "   ‚Ä¢ MinIO:         http://localhost:9001"
    echo "   ‚Ä¢ Gitea:         http://localhost:3000"
    echo "   ‚Ä¢ Spark Master:  http://localhost:8085"
    echo "   ‚Ä¢ Datagen:       http://localhost:8000"
    echo ""
    echo "üìã Tabelas Iceberg (com dados!):"
    echo "   ‚Ä¢ iceberg.isp.customers  - 100 registros"
    echo "   ‚Ä¢ iceberg.isp.sessions   - 500 registros"
    echo "   ‚Ä¢ iceberg.isp.invoices   - 200 registros"
    echo "   ‚Ä¢ iceberg.isp.contracts  - 100 registros"
    echo ""
}

main "$@"
